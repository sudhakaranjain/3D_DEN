{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#To increase cell width of ipynb\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEN():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.last_label_index = 0    # tracks the label index upto which the model has been trained\n",
    "        self.params = dict()\n",
    "        self.k_ex = 5\n",
    "        self.den_layers = 3\n",
    "        self.conv_layers = 2\n",
    "        tf.reset_default_graph()\n",
    "        self.sess = None\n",
    "        self.train = []\n",
    "        self.train_labels = []\n",
    "        self.test = []\n",
    "        self.test_labels = []\n",
    "        self.batch_size = 10\n",
    "        self.epochs = 50\n",
    "        self.early_stop = 5\n",
    "        self.selected = dict()\n",
    "        self.lr = 1e-3\n",
    "        self.l2_mu = 0.001\n",
    "        self.lamba_regular = 0.7\n",
    "        self.l1_thr = 0.000001\n",
    "        self.loss_thr = 0.01\n",
    "\n",
    "    def extract_data(self):\n",
    "        train = []\n",
    "        test = []\n",
    "        train_labels = []\n",
    "        test_labels = []\n",
    "        label_names = []\n",
    "        \n",
    "        # For easier disk read operation\n",
    "#         VGG16\n",
    "#         np_train_path = '../ModelNet40/extracted_features/VGG16/train.npy.gz'\n",
    "#         np_test_path = '../ModelNet40/extracted_features/VGG16/test.npy.gz'\n",
    "#         np_train_label_path = '../ModelNet40/extracted_features/VGG16/train_labels.npy'\n",
    "#         np_test_label_path = '../ModelNet40/extracted_features/VGG16/test_labels.npy'\n",
    "#         np_label_names_path = '../ModelNet40/extracted_features/VGG16/label_names.npy'\n",
    "\n",
    "#       MobileNetV2\n",
    "#         np_train_path = '../ModelNet40/extracted_features/MobileNetV2/train.npy.gz'\n",
    "#         np_test_path = '../ModelNet40/extracted_features/MobileNetV2/test.npy.gz'\n",
    "#         np_train_label_path = '../ModelNet40/extracted_features/MobileNetV2/train_labels.npy'\n",
    "#         np_test_label_path = '../ModelNet40/extracted_features/MobileNetV2/test_labels.npy'\n",
    "#         np_label_names_path = '../ModelNet40/extracted_features/MobileNetV2/label_names.npy'\n",
    "        \n",
    "#       Custom MobileNetV2\n",
    "        np_train_path = '../ModelNet40/extracted_features/custom_MobileNetV2/train.npy.gz'\n",
    "        np_test_path = '../ModelNet40/extracted_features/custom_MobileNetV2/test.npy.gz'\n",
    "        np_train_label_path = '../ModelNet40/extracted_features/custom_MobileNetV2/train_labels.npy'\n",
    "        np_test_label_path = '../ModelNet40/extracted_features/custom_MobileNetV2/test_labels.npy'\n",
    "        np_label_names_path = '../ModelNet40/extracted_features/custom_MobileNetV2/label_names.npy'\n",
    "        \n",
    "        print(\"\\n..........loading dataset from numpy files..........\\n\")\n",
    "\n",
    "        with gzip.GzipFile(np_train_path, \"r\") as f:\n",
    "            train = np.load(f)\n",
    "        with gzip.GzipFile(np_test_path, \"r\") as f:\n",
    "            test = np.load(f)\n",
    "\n",
    "        train_labels = np.load(np_train_label_path)\n",
    "        test_labels = np.load(np_test_label_path)\n",
    "        label_names = np.load(np_label_names_path)\n",
    "\n",
    "        return train, train_labels, test, test_labels, label_names\n",
    "\n",
    "\n",
    "    def add_task(self, task_id, label_names, initial_output=2):\n",
    "\n",
    "        new_label_indices = []\n",
    "        self.train = []\n",
    "        self.train_labels = []\n",
    "        self.train_ex_labels = []      \n",
    " \n",
    "        if task_id == 1:\n",
    "            self.new_train = []\n",
    "            self.new_train_labels = []\n",
    "            self.new_train_ex_labels = []\n",
    "            self.total = []\n",
    "            self.total_ex_labels = []\n",
    "            self.test = []\n",
    "            self.test_labels = []\n",
    "            for i in range(initial_output):   # By default, first task is a binary classification\n",
    "                new_label_indices.append(i)\n",
    "            self.last_label_index = i\n",
    "\n",
    "        else:\n",
    "            new_label_indices.append(self.last_label_index+1)\n",
    "            self.last_label_index = self.last_label_index + 1\n",
    "            \n",
    "            #saving the old training data\n",
    "            self.total = self.total + self.new_train\n",
    "            self.total_ex_labels = self.total_ex_labels + self.new_train_ex_labels\n",
    "            self.new_train = []\n",
    "            self.new_train_labels = []\n",
    "            self.new_train_ex_labels = []\n",
    "\n",
    "        for index in new_label_indices:\n",
    "            print(\" \\n Added new category: \"+str(label_names[index]))\n",
    "            l = 1 if task_id != 1 else index\n",
    "            for data, label in zip(train_data, train_labels):\n",
    "                if label_names[index] == label:\n",
    "                    self.new_train.append(data)\n",
    "                    self.new_train_labels.append(l)\n",
    "                    self.new_train_ex_labels.append(index)\n",
    "#                 if len(self.new_train_labels) >= 300:\n",
    "#                     break\n",
    "\n",
    "            for data, label in zip(test_data, test_labels):\n",
    "                if label_names[index] == label:\n",
    "                    self.test.append(data)\n",
    "                    self.test_labels.append(index)\n",
    "                    \n",
    "        # ------------------ Random sampling old training data ------------------\n",
    "#         if task_id != 1:\n",
    "#             sampled_indices = random.sample(range(len(self.total)), len(self.new_train))\n",
    "#             for k in sampled_indices:\n",
    "#                 self.train.append(self.total[k])\n",
    "#                 self.train_labels.append(0)\n",
    "#                 self.train_ex_labels.append(self.total_ex_labels[k])\n",
    "                \n",
    "#             self.train = self.train + self.new_train\n",
    "#             self.train_labels = self.train_labels + self.new_train_labels\n",
    "#             self.train_ex_labels = self.train_ex_labels + self.new_train_ex_labels\n",
    "#         else:\n",
    "#             self.train = self.new_train\n",
    "#             self.train_labels = self.new_train_labels\n",
    "#             self.train_ex_labels = self.new_train_ex_labels\n",
    "        \n",
    "        # ------------------- Memory Replay of old training data ----------------------\n",
    "        if task_id != 1:\n",
    "            sample_per_label = int(len(self.new_train) / self.last_label_index)\n",
    "            if sample_per_label < 20:\n",
    "                sample_per_label = 20\n",
    "                \n",
    "            for index in range(self.last_label_index):\n",
    "                label_indices = []\n",
    "                for i, m in enumerate(self.total_ex_labels):\n",
    "                    if m == index:\n",
    "                        label_indices.append(i)\n",
    "                try:\n",
    "                    sampled_label_indices = random.sample(label_indices, k=sample_per_label)\n",
    "                except:\n",
    "                    print(\"\\n Less samples; so taking them all\")\n",
    "                    sampled_label_indices = label_indices\n",
    "                for k in sampled_label_indices:\n",
    "                    self.train.append(self.total[k])\n",
    "                    self.train_labels.append(0)\n",
    "                    self.train_ex_labels.append(self.total_ex_labels[k])\n",
    "                    \n",
    "            self.train = self.train + self.new_train\n",
    "            self.train_labels = self.train_labels + self.new_train_labels\n",
    "            self.train_ex_labels = self.train_ex_labels + self.new_train_ex_labels\n",
    "\n",
    "        else:\n",
    "            self.train = self.new_train\n",
    "            self.train_labels = self.new_train_labels\n",
    "            self.train_ex_labels = self.new_train_ex_labels\n",
    "        \n",
    "        print(\"Train size: %d\" %len(self.train_ex_labels))\n",
    "        print(\"Test size: %d\" %len(self.test_labels))\n",
    "            \n",
    "\n",
    "    def destroy_graph(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.params = dict()\n",
    "\n",
    "    def initialize_parameters(self, output_len=2):\n",
    "        self.sess = tf.Session()\n",
    "#       MobileNetV2 :-\n",
    "        self.x = tf.placeholder(tf.float32, [None, 4 * 4 * 1280])\n",
    "#       VGG16 :-\n",
    "#         self.x = tf.placeholder(tf.float32, [None, 4 * 4 * 512])\n",
    "        self.y_ = tf.placeholder(tf.float32, [None, output_len])\n",
    "        # self.keep_prob = tf.placeholder(tf.float32)          # dropout probability\n",
    "\n",
    "    def create_variable(self, name=None, shape=None, scope=None, trainable=True):\n",
    "        with tf.variable_scope(scope, reuse=False):\n",
    "            w = tf.get_variable(name, shape=shape, \n",
    "#                                 initializer=tf.random_normal_initializer(mean=0, stddev=1, seed=3),\n",
    "                                trainable=trainable)\n",
    "            if \"ex\" not in name:\n",
    "                self.params[w.name] = w\n",
    "        return w\n",
    "\n",
    "    def get_variable(self, name=None, scope=None):\n",
    "        with tf.variable_scope(scope, reuse=True):\n",
    "            w = tf.get_variable(name)\n",
    "            if \"ex\" not in name:\n",
    "                self.params[w.name] = w\n",
    "        return w\n",
    "\n",
    "    def restore_params(self, task_id=None, trainable=True, param_values=dict()):\n",
    "        self.params = dict()\n",
    "        self.prev_W = dict()\n",
    "        for scope_name, value in param_values.items():\n",
    "            self.prev_W[scope_name] = value\n",
    "            scope_name = scope_name.split(':')[0]\n",
    "            [scope, name] = scope_name.split('/')\n",
    "\n",
    "#             if task_id != None:\n",
    "#                 if ('l%d/w_%d' % (self.den_layers,task_id) in scope_name) or ('l%d/b_%d' % (self.den_layers,task_id) in scope_name):\n",
    "#                     trainable = True\n",
    "#                 else:\n",
    "#                     trainable = False\n",
    "\n",
    "            with tf.variable_scope(scope, reuse=False):\n",
    "                w = tf.get_variable(name, initializer=value, trainable=trainable)\n",
    "            self.params[w.name] = w\n",
    "\n",
    "    def get_params(self):\n",
    "        vdict = dict()\n",
    "        for scope_name, ref_w in self.params.items():\n",
    "            vdict[scope_name] = self.sess.run(ref_w)\n",
    "        return vdict\n",
    "\n",
    "    def conv2d(self, x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    def max_pool_2x2(self, x):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "    def build_model(self, task_id, expansion=False, output_len=2):\n",
    "\n",
    "        # Note: scope and name values are only given to DEN layers, not for fixed sized layers.\n",
    "\n",
    "        self.initialize_parameters(output_len)\n",
    "\n",
    "        if task_id == 1:\n",
    "\n",
    "            #flattened first fc layer\n",
    "            W_fc1 = self.create_variable(name=\"w\", shape=[4 * 4 * 1280, 1024], scope=\"l1\")   # (MobileNetV2) layer-1 outgoing weight matrix\n",
    "#             W_fc1 = self.create_variable(name=\"w\", shape=[4 * 4 * 512, 1024], scope=\"l1\")   # (VGG16) layer-1 outgoing weight matrix\n",
    "            b_fc1 = self.create_variable(name=\"b\", shape=[1024], scope=\"l1\")\n",
    "\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(self.x, W_fc1) + b_fc1)\n",
    "\n",
    "            #second fc layer\n",
    "            W_fc2 = self.create_variable(name=\"w\", shape=[1024, 256], scope=\"l2\")   # layer-2 outgoing weight matrix\n",
    "            b_fc2 = self.create_variable(name=\"b\", shape=[256], scope=\"l2\")\n",
    "\n",
    "            self.h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "\n",
    "            # readout fc layer\n",
    "            self.w_fc = self.create_variable(name=\"w\", shape=[256, output_len], scope=\"l3\")     # layer-3 outgoing weight matrix \n",
    "            self.b_fc = self.create_variable(name=\"b\", shape=[output_len], scope=\"l3\")\n",
    "            y_conv = tf.matmul(self.h_fc2, self.w_fc) + self.b_fc\n",
    "            \n",
    "        elif expansion:\n",
    "            \n",
    "            # fc layer expansion\n",
    "            for layer in range(1, self.den_layers+1):\n",
    "\n",
    "                if layer == 1:\n",
    "                    w_fc1 = self.get_variable(name=\"w\", scope=\"l%d\"%layer)\n",
    "                    b_fc1 = self.get_variable(name=\"b\", scope=\"l%d\"%layer)\n",
    "\n",
    "                    w_expand = self.create_variable(name=\"w_ex_\"+str(task_id),shape=[w_fc1.get_shape().as_list()[0], self.k_ex], scope=\"l\"+str(layer))\n",
    "                    b_expand = self.create_variable(name=\"b_ex_\"+str(task_id),shape=[self.k_ex], scope=\"l%d\"%layer)\n",
    "                    w_expanded = tf.concat([w_fc1,w_expand],1)\n",
    "                    b_expanded = tf.concat([b_fc1,b_expand],0)\n",
    "                    self.params[w_fc1.name] = w_expanded\n",
    "                    self.params[b_fc1.name] = b_expanded\n",
    "                    h_fc1 = tf.nn.relu(tf.matmul(self.x, w_expanded) + b_expanded)\n",
    "\n",
    "                elif layer == self.den_layers:\n",
    "                    #weight matrix of current task output\n",
    "                    w_fc3 = self.get_variable(name=\"w\",scope=\"l%d\"%layer)\n",
    "                    b_fc3 = self.get_variable(name=\"b\",scope=\"l%d\"%layer)\n",
    "                    next_dim = w_fc3.get_shape().as_list()[1]\n",
    "                    \n",
    "                    w_expand = self.create_variable(name=\"w_ex_\"+str(task_id), shape=[self.k_ex, next_dim], scope=\"l%d\"%layer)\n",
    "                    w_expanded = tf.concat([w_fc3, w_expand], 0)\n",
    "                    \n",
    "                    self.params[w_fc3.name] = w_expanded\n",
    "                    \n",
    "                    y_conv = tf.matmul(self.h_fc2, w_expanded) + b_fc3\n",
    "\n",
    "                else:\n",
    "                    w_fc2 = self.get_variable(name=\"w\",scope=\"l%d\"%layer)\n",
    "                    b_fc2 = self.get_variable(name=\"b\",scope=\"l%d\"%layer)\n",
    "\n",
    "                    prev_dim = w_fc2.get_shape().as_list()[0]\n",
    "                    next_dim = w_fc2.get_shape().as_list()[1]\n",
    "                    \n",
    "                    # Dummy nodes for prev hidden nodes\n",
    "                    dummy_w = tf.get_variable(name=\"dummy_t%d_l%d\" %(task_id,layer), shape=[self.k_ex, next_dim], \n",
    "                                initializer=tf.constant_initializer(0.0), trainable=False)\n",
    "\n",
    "                    w_expand = self.create_variable(name=\"w_ex_\"+str(task_id), shape=[prev_dim + self.k_ex, self.k_ex], scope=\"l%d\"%layer)\n",
    "                    b_expand = self.create_variable(name=\"b_ex_\"+str(task_id), shape=[self.k_ex], scope=\"l%d\"%layer)\n",
    "                    \n",
    "                    w_fc2_dummy = tf.concat([w_fc2, dummy_w],0)\n",
    "                    \n",
    "                    w_expanded = tf.concat([w_fc2_dummy, w_expand], 1)\n",
    "                    b_expanded = tf.concat([b_fc2, b_expand], 0)\n",
    "                    \n",
    "                    self.params[w_fc2.name] = w_expanded\n",
    "                    self.params[b_fc2.name] = b_expanded\n",
    "                    self.h_fc2 = tf.nn.relu(tf.matmul(h_fc1, w_expanded) + b_expanded)\n",
    "                    \n",
    "        else:\n",
    "\n",
    "            #flattened first fc layer\n",
    "            W_fc1 = self.get_variable(name=\"w\", scope=\"l1\")   # layer-1 outgoing weight matrix\n",
    "            b_fc1 = self.get_variable(name=\"b\", scope=\"l1\")\n",
    "\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(self.x, W_fc1) + b_fc1)\n",
    "\n",
    "            #second fc layer\n",
    "            W_fc2 = self.get_variable(name=\"w\", scope=\"l2\")   # layer-2 outgoing weight matrix\n",
    "            b_fc2 = self.get_variable(name=\"b\", scope=\"l2\")\n",
    "\n",
    "            self.h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "\n",
    "            self.w_fc = self.create_variable(name=\"w_\"+str(task_id), shape=[self.h_fc2.shape[1], output_len], scope=\"l3\", trainable=True)     # layer-3 outgoing weight matrix \n",
    "            self.b_fc = self.create_variable(name=\"b_\"+str(task_id), shape=[output_len], scope=\"l3\", trainable=True)\n",
    "            y_conv = tf.matmul(self.h_fc2, self.w_fc) + self.b_fc\n",
    "\n",
    "#         y_conv = tf.nn.sigmoid(y_conv)\n",
    "        return y_conv\n",
    "    \n",
    "    def perform_selection(self, task_id, values_dict):      # Breadth first search for selecting non-zero units\n",
    "        \n",
    "        all_indices = defaultdict(list)     # to store indices of nonzero units\n",
    "        selected_params = dict()            # to store values of nonzero units\n",
    "        selected_prev_params = dict()\n",
    "        \n",
    "#         for scope, value in values_dict.items():    # Storing conv layers in selected parameters \n",
    "#             if \"conv\" in scope:\n",
    "#                 selected_params[scope] = value\n",
    "            \n",
    "        for i in reversed(range(1,self.den_layers+1)):\n",
    "            if i == self.den_layers:\n",
    "                w = values_dict['l%d/w:0' % i]\n",
    "                b = values_dict['l%d/b:0' % i]\n",
    "                w_t = values_dict['l%d/w_%d:0' %(i,task_id)]\n",
    "                b_t = values_dict['l%d/b_%d:0' %(i,task_id)]\n",
    "                w = np.concatenate((w, w_t), 1)\n",
    "                b = np.concatenate((b, b_t), 0)\n",
    "                for j in range(w.shape[0]):\n",
    "                    if w[j,-1] <= 0.0001:\n",
    "                        all_indices['l%d' % i].append(j)\n",
    "                # np.ix_(): fancy indexing, index with arrays of integers\n",
    "                # Select non-zero weights between the last hidden layer and the output layer\n",
    "                selected_params['l%d/w_%d:0' % (i, task_id)] = \\\n",
    "                    w[np.ix_(all_indices['l%d' % i], list(range(w.shape[1])))]\n",
    "                selected_params['l%d/b_%d:0' % (i, task_id)] = b\n",
    "            else:\n",
    "                w = values_dict['l%d/w:0' % i]\n",
    "                b = values_dict['l%d/b:0' % i]\n",
    "                top_indices = all_indices['l%d' % (i + 1)]\n",
    "                print(\"Layer %d: Selecting %d neurons\" %(i+1, len(top_indices)))\n",
    "                for j in range(w.shape[0]):\n",
    "                    if np.count_nonzero(w[j, top_indices] <= 0.0001) != 0 or i == 1:\n",
    "                        all_indices['l%d' % i].append(j)\n",
    "                \n",
    "                # non-zero weights between the layer i and the layer i+1\n",
    "                sub_weight = w[np.ix_(all_indices['l%d' % i], top_indices)]\n",
    "                sub_biases = b[all_indices['l%d' % (i + 1)]]\n",
    "                selected_params['l%d/w:0' % i] = sub_weight\n",
    "                selected_params['l%d/b:0' % i] = sub_biases\n",
    "                \n",
    "                # prev_W : to avoid drastic change in value of weights (Regularization)\n",
    "                selected_prev_params['l%d/w:0' % i] = \\\n",
    "                    self.prev_W['l%d/w:0' % i][np.ix_(all_indices['l%d' % i], top_indices)]\n",
    "                selected_prev_params['l%d/b:0' % i] = \\\n",
    "                    self.prev_W['l%d/b:0' % i][all_indices['l%d' % (i + 1)]]\n",
    "\n",
    "#         for keys, value in selected_params.items():\n",
    "#             print(keys)\n",
    "#             print(value)\n",
    "                \n",
    "        return [selected_params, selected_prev_params, all_indices]\n",
    "        \n",
    "    def build_SR(self, task_id, selected, output_len):    # creating selective retraining model\n",
    "        \n",
    "        self.initialize_parameters(output_len)\n",
    "        h = self.x\n",
    "        \n",
    "        for i in range(1, self.den_layers):\n",
    "            with tf.variable_scope('l%d' % i):\n",
    "                w = tf.get_variable('w', initializer=selected['l%d/w:0' % i], trainable=True)\n",
    "                b = tf.get_variable('b', initializer=selected['l%d/b:0' % i], trainable=True)\n",
    "            h = tf.nn.relu(tf.matmul(h, w) + b)\n",
    "            \n",
    "        # last layer\n",
    "        with tf.variable_scope('l%d' % self.den_layers):\n",
    "            w = tf.get_variable('w_%d' % task_id,\n",
    "                                initializer=selected['l%d/w_%d:0' % (self.den_layers, task_id)], trainable=True)\n",
    "            b = tf.get_variable('b_%d' % task_id,\n",
    "                                initializer=selected['l%d/b_%d:0' % (self.den_layers, task_id)], trainable=True)\n",
    "\n",
    "        y_conv = tf.matmul(h, w) + b\n",
    "#         y_conv = tf.nn.sigmoid(y_conv)\n",
    "        return y_conv\n",
    "\n",
    "    def optimization(self, prev_W=None):\n",
    "\n",
    "        l2_regular = 0\n",
    "        train_var = []\n",
    "        regular_terms = []\n",
    "        \n",
    "        self.loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=self.y_, logits=y_conv))\n",
    "\n",
    "#         for var in tf.trainable_variables():\n",
    "#             l2_regular = l2_regular + tf.nn.l1_loss(var)\n",
    "#             train_var.append(var)\n",
    "#         print(len(train_var))\n",
    "\n",
    "        l1_var = [var for var in tf.trainable_variables()]\n",
    "        regularizer = tf.contrib.layers.l1_regularizer(self.l2_mu)\n",
    "        reg_term = tf.contrib.layers.apply_regularization(regularizer, l1_var)\n",
    "        self.loss = self.loss + reg_term\n",
    "#         self.loss = self.loss + tf.reduce_mean(self.l2_mu * l2_regular)\n",
    "\n",
    "        if prev_W != None:\n",
    "            for var in l1_var:\n",
    "                if var.name in prev_W.keys():\n",
    "                    prev_w = prev_W[var.name]\n",
    "                    regular_terms.append(tf.nn.l2_loss(var - prev_w))\n",
    "            self.loss = self.loss + self.lamba_regular * tf.reduce_mean(regular_terms)\n",
    "        \n",
    "        opt = tf.train.AdamOptimizer(self.lr)\n",
    "        grads = opt.compute_gradients(self.loss, l1_var)\n",
    "        apply_grads = opt.apply_gradients(grads)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(self.y_,1))\n",
    "        self.acc_train = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "#         l1_var = [var for var in tf.trainable_variables()]\n",
    "        l1_op_list = []\n",
    "        with tf.control_dependencies([apply_grads]):  # exec apply_grads first\n",
    "            for var in tf.trainable_variables():\n",
    "                th_t = tf.fill(tf.shape(var), tf.convert_to_tensor(self.l1_thr))\n",
    "                zero_t = tf.zeros(tf.shape(var))\n",
    "#                 var_temp = var - (th_t * tf.sign(var))\n",
    "                # [pseudo]:  if |var| < th_t: var = [0];  else: var = var_temp\n",
    "                l1_op = var.assign(tf.where(tf.less(tf.abs(var), th_t), zero_t, var))\n",
    "                l1_op_list.append(l1_op)\n",
    "        \n",
    "        with tf.control_dependencies(l1_op_list):\n",
    "            self.train_model = tf.no_op()\n",
    "\n",
    "    def train_task(self, task_id, batch_size, epochs, retraining=False, expansion=False):\n",
    "\n",
    "        if task_id == 1:\n",
    "            task_train_labels = tf.one_hot(indices=np.array(self.train_labels), depth=self.last_label_index+1)\n",
    "        elif retraining == True or expansion == True:\n",
    "            task_train_labels = tf.one_hot(indices=np.array(self.train_ex_labels), depth=self.last_label_index+1)\n",
    "        else:\n",
    "            task_train_labels = np.array(self.train_labels)\n",
    "            task_train_labels = task_train_labels.reshape((task_train_labels.shape[0],1))\n",
    "            task_train_labels = tf.convert_to_tensor(task_train_labels)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((np.array(self.train), task_train_labels))\n",
    "        dataset = dataset.shuffle(len(self.train_ex_labels)).repeat().batch(batch_size)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        (x_data , y_data) = iterator.get_next()\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        task_train_labels = self.sess.run(task_train_labels)\n",
    "        count = 0\n",
    "\n",
    "#         for scope, ref in self.params.items():\n",
    "#             if \"l2\" in scope:\n",
    "#                 print(self.sess.run(ref))\n",
    "\n",
    "        for i in range(epochs):\n",
    "\n",
    "            for j in range(int(len(self.train)/batch_size)):\n",
    "                x_batch , y_batch = self.sess.run([x_data,y_data])\n",
    "                _, loss = self.sess.run([self.train_model, self.loss], feed_dict={self.x: x_batch, self.y_: y_batch})\n",
    "\n",
    "            train_accuracy = self.acc_train.eval(session=self.sess, feed_dict={self.x: self.train, self.y_: task_train_labels})\n",
    "            print(\"Epoch %d, training accuracy %g\"%(i+1, train_accuracy))\n",
    "\n",
    "            if train_accuracy == 1:\n",
    "                count += 1\n",
    "\n",
    "                if count > 9:\n",
    "                    print(\"Best accuracy achieved! \\n\")\n",
    "                    break\n",
    "        \n",
    "#         for scope, ref in self.params.items():\n",
    "#             if \"l2\" in scope:\n",
    "#                 print(self.sess.run(ref))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def predict(self, task_id, output_len=2):\n",
    "\n",
    "        self.initialize_parameters(output_len)\n",
    "\n",
    "        task_test_labels = tf.one_hot(indices=self.test_labels, depth=self.last_label_index+1)\n",
    "        task_test_labels = self.sess.run(task_test_labels)\n",
    "        \n",
    "        #flattened first fc layer\n",
    "        W_fc1 = self.get_variable(name=\"w\", scope=\"l1\")   # layer-1 outgoing weight matrix\n",
    "        b_fc1 = self.get_variable(name=\"b\", scope=\"l1\")\n",
    "\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(self.x, W_fc1) + b_fc1)\n",
    "\n",
    "        #second fc layer\n",
    "        W_fc2 = self.get_variable(name=\"w\", scope=\"l2\")   # layer-2 outgoing weight matrix\n",
    "        b_fc2 = self.get_variable(name=\"b\", scope=\"l2\")\n",
    "\n",
    "        self.h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "\n",
    "        self.w_fc = self.get_variable(name=\"w\", scope=\"l3\")\n",
    "        self.b_fc = self.get_variable(name=\"b\", scope=\"l3\")\n",
    "\n",
    "        y_final = tf.matmul(self.h_fc2, self.w_fc) + self.b_fc\n",
    "        y_final = tf.nn.softmax(y_final)\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(y_final,1), tf.argmax(self.y_,1))\n",
    "        self.acc_test = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        test_accuracy = self.acc_test.eval(session=self.sess, feed_dict={self.x: self.test, self.y_: task_test_labels})\n",
    "        return test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    den = DEN()\n",
    "\n",
    "    train_data, train_labels, test_data, test_labels, label_names = den.extract_data()\n",
    "    np.random.shuffle(label_names)\n",
    "\n",
    "# show image using cv\n",
    "    # print(train_labels[512])\n",
    "    # cv.imshow(\"\", train_data[512])\n",
    "    # cv.waitKey(0)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    task_id = 0\n",
    "    y_conv = None\n",
    "\n",
    "    while den.last_label_index != (len(label_names) - 1):     # Loop for adding new tasks (lifelong learning)\n",
    "\n",
    "        task_id += 1\n",
    "        den.add_task(task_id, label_names)\n",
    "        param_values = dict()\n",
    "        selected = dict()\n",
    "        print(\"-------------Training new task: %d--------------\"%task_id)\n",
    "        if task_id == 1:\n",
    "            y_conv = den.build_model(task_id=task_id)\n",
    "            den.optimization()\n",
    "            _ = den.train_task(task_id=task_id, batch_size=den.batch_size, epochs=den.epochs)\n",
    "            params = den.get_params()\n",
    "\n",
    "            den.destroy_graph()\n",
    "            den.restore_params(trainable=False, param_values=params)  # Freezes all learned weights making them non-trainable\n",
    "\n",
    "            # ---------Performance-----------\n",
    "            test_accuracy = den.predict(task_id=task_id, output_len=den.last_label_index+1)\n",
    "            \n",
    "        else:\n",
    "            print(\"-----------Started Selective Retraining-------------\")\n",
    "            \n",
    "            #------------------Selection-------------------\n",
    "            print(\"---- Selecting nodes ----\")\n",
    "            y_conv = den.build_model(task_id=task_id, output_len=1)\n",
    "            den.sess.run(tf.global_variables_initializer())\n",
    "            params = den.get_params()\n",
    "            den.optimization()\n",
    "            \n",
    "            _ = den.train_task(task_id=task_id, batch_size=den.batch_size, epochs=den.early_stop)\n",
    "            params = den.get_params()\n",
    "       \n",
    "            [selected, selected_prev, all_indices] = den.perform_selection(task_id=task_id, values_dict=params)\n",
    "            den.destroy_graph()\n",
    "\n",
    "            #------------------Retraining-------------------\n",
    "            print(\"\\n---- Retraining selected nodes ----\")\n",
    "            y_conv = den.build_SR(task_id=task_id, selected=selected, output_len=den.last_label_index+1) \n",
    "            den.optimization(prev_W=selected_prev) \n",
    "            loss = den.train_task(task_id=task_id, batch_size=den.batch_size, epochs=den.epochs, retraining=True)\n",
    "\n",
    "            print(\"Loss: %f\"%loss)\n",
    "\n",
    "            #--------------Performing Union----------------\n",
    "            _vars = [(var.name, den.sess.run(var)) for var in tf.trainable_variables() if 'l' in var.name]\n",
    "\n",
    "            for item in _vars:\n",
    "                key, values = item\n",
    "                selected[key] = values\n",
    "\n",
    "            for i in reversed(range(1, den.den_layers+1)):\n",
    "                if i == den.den_layers:\n",
    "                    temp_weight = np.concatenate((params['l%d/w:0' % i], params['l%d/w_%d:0' % (i, task_id)]), axis=1)\n",
    "                    temp_weight[np.ix_(all_indices['l%d' % i], list(range(den.last_label_index+1)))] = \\\n",
    "                        selected['l%d/w_%d:0' % (i, task_id)]\n",
    "                    # Updating output matrix structure\n",
    "                    params['l%d/w:0' % (i)] = temp_weight.tolist()\n",
    "                    params['l%d/b:0' % (i)] = \\\n",
    "                        selected['l%d/b_%d:0' % (i, task_id)]\n",
    "\n",
    "#                         params['l%d/w:0' % (i)] = np.concatenate([params['l%d/w:0' % (i)],params['l%d/w_%d:0' % (i,task_id)]], axis=1).tolist()\n",
    "#                         params['l%d/b:0' % (i)] = np.concatenate([params['l%d/b:0' % (i)],params['l%d/b_%d:0' % (i,task_id)]], axis=0).tolist()\n",
    "                else:\n",
    "                    temp_weight = params['l%d/w:0' % i]\n",
    "                    temp_biases = params['l%d/b:0' % i]\n",
    "                    temp_weight[np.ix_(all_indices['l%d' % i], all_indices['l%d' % (i + 1)])] = \\\n",
    "                        selected['l%d/w:0' % i]\n",
    "                    temp_biases[all_indices['l%d' % (i + 1)]] = \\\n",
    "                        selected['l%d/b:0' % i]\n",
    "                    params['l%d/w:0' % i] = temp_weight\n",
    "                    params['l%d/b:0' % i] = temp_biases\n",
    "\n",
    "            den.destroy_graph()\n",
    "            den.restore_params(trainable=False, param_values=params)  # Freezes all learned weights making them non-trainable\n",
    "\n",
    "            # ---------Performance-----------\n",
    "            test_accuracy = den.predict(task_id=task_id, output_len=den.last_label_index+1)\n",
    "            \n",
    "            if test_accuracy < 0.85:\n",
    "                \n",
    "                print(\"Overall accuracy: %g, lesser than threshold\" %test_accuracy)\n",
    "                print(\"-----------Started Dynamic Expansion------------\")\n",
    "                y_conv = den.build_model(task_id=task_id, expansion=True, output_len=den.last_label_index+1)\n",
    "                den.optimization()\n",
    "                _ = den.train_task(task_id=task_id, batch_size=den.batch_size, epochs=den.epochs, expansion=True)\n",
    "                params = den.get_params()\n",
    "                \n",
    "                den.destroy_graph()\n",
    "                den.restore_params(trainable=False, param_values=params)  # Freezes all learned weights making them non-trainable\n",
    "                \n",
    "                test_accuracy = den.predict(task_id=task_id, output_len=den.last_label_index+1)\n",
    "        \n",
    "        print(\"Final accuracy: %g \\n\" %test_accuracy)     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
