{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#To increase cell width of ipynb\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# from collections import defaultdict\n",
    "import gzip\n",
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tfutils as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gf_dim = 64\n",
    "        self.df_dim = 64\n",
    "        self.height = 128\n",
    "        self.width = 128\n",
    "        self.channel = 3\n",
    "        self.z_dim = 128\n",
    "        self.lr = 1e-4\n",
    "        self.beta1 = 0.5\n",
    "        self.sample_num = 5\n",
    "        tf.reset_default_graph()\n",
    "        self.sess = tf.Session()\n",
    "        self.batch_size = 10\n",
    "        # Placeholders\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, self.height, self.width, self.channel], name='x-images')\n",
    "        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name='z-noise')\n",
    "\n",
    "        self.IMG_SHAPE = (self.width, self.height, 3)\n",
    "    \n",
    "    def extract_data(self):\n",
    "        train = []\n",
    "        test = []\n",
    "        \n",
    "        if os.path.exists(np_train_path):\n",
    "            \n",
    "            print(\"\\n..........loading dataset from numpy files..........\\n\")\n",
    "            \n",
    "            with gzip.GzipFile(np_train_path, \"r\") as f:\n",
    "                train = np.load(f)\n",
    "        else:\n",
    "            \n",
    "            print(\"\\n..........loading dataset from disk..........\\n\")\n",
    "            \n",
    "            for file in os.listdir(\"./airplane\"):\n",
    "                file_path = os.path.join(\"./airplane\", file)\n",
    "                if str(file) == \"train\":\n",
    "                    for label in sorted(os.listdir(file_path)):\n",
    "                        label_path = os.path.join(file_path, label)\n",
    "                        for img in os.listdir(label_path):\n",
    "                            img_path = os.path.join(label_path, img)\n",
    "                            if str(img[0:-4]) == \"1\":\n",
    "                                train1 = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "                            elif str(img[0:-4]) == \"6\":\n",
    "                                train2 = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "                            elif str(img[0:-4]) == \"10\":\n",
    "                                train3 = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "                        merged = [train1, train2, train3]\n",
    "                        random.shuffle(merged)\n",
    "                        image1 = cv.merge(tuple(merged))\n",
    "#                         image1 = cv.merge((train1, train2, train3))\n",
    "                        train.append(image1)\n",
    "        #                 cv.imshow(\"image\", image)\n",
    "        #                 cv.waitKey(0)\n",
    "        #                 cv.destroyAllWindows()\n",
    "        #                 cv.imwrite(\"./2.jpg\", image1)\n",
    "        #                 break\n",
    "\n",
    "    #             elif str(file) == \"test\":\n",
    "    #                 for label in sorted(os.listdir(file_path)):\n",
    "    #                     label_path = os.path.join(file_path, label)\n",
    "    #                     for img in os.listdir(label_path):\n",
    "    #                         img_path = os.path.join(label_path, img)\n",
    "    #                         if str(img[0:-4]) == \"1\":\n",
    "    #                             test1 = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "    #                         elif str(img[0:-4]) == \"6\":\n",
    "    #                             test2 = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "    #                         elif str(img[0:-4]) == \"10\":\n",
    "    #                             test3 = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "    #                     image2 = cv.merge((test1, test2, test3))\n",
    "    #                     test.append(image2)\n",
    "            train = np.array(train, dtype=\"float\") / 255.0\n",
    "    #         test = np.array(test, dtype=\"float\") / 255.0\n",
    "            train = train.reshape(train.shape[0], 128, 128, 3)\n",
    "    #         test = test.reshape(test.shape[0], 128, 128, 3)\n",
    "\n",
    "            with gzip.GzipFile(np_train_path, \"w\") as f:\n",
    "                np.save(f, train)\n",
    "    #         with gzip.GzipFile(np_test_path, \"w\") as f:\n",
    "    #             np.save(f, test)\n",
    "        return train\n",
    "    \n",
    "    # gf_dim * value represents total number of conv filters in that layer, i.e, 64 * 8, 64 * 4, etc.\n",
    "    def generator(self, z, reuse=None, is_train=True):\n",
    "        with tf.variable_scope('generator', reuse=reuse):\n",
    "            x = t.dense(z, self.gf_dim * 16 * 4 * 4, name='gen-fc-1')\n",
    "\n",
    "            x = tf.reshape(x, [-1, 4, 4, self.gf_dim * 16])\n",
    "            x = t.batch_norm(x, is_train=is_train, name='gen-bn-1')\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            x = t.deconv2d(x, self.gf_dim * 8, 5, 2, name='gen-deconv2d-1')\n",
    "            x = t.batch_norm(x, is_train=is_train, name='gen-bn-2')\n",
    "            x = tf.nn.relu(x)\n",
    "#             print(x)\n",
    "\n",
    "            x = t.deconv2d(x, self.gf_dim * 4, 5, 2, name='gen-deconv2d-2')\n",
    "            x = t.batch_norm(x, is_train=is_train, name='gen-bn-3')\n",
    "            x = tf.nn.relu(x)\n",
    "#             print(x)\n",
    "\n",
    "            x = t.deconv2d(x,  self.gf_dim * 2, 5, 2, name='gen-deconv2d-3')\n",
    "            x = t.batch_norm(x, is_train=is_train, name='gen-bn-4')\n",
    "            x = tf.nn.relu(x)\n",
    "#             print(x)\n",
    "\n",
    "            x = t.deconv2d(x,  self.gf_dim * 1, 5, 2, name='gen-deconv2d-4')\n",
    "            x = t.batch_norm(x, is_train=is_train, name='gen-bn-5')\n",
    "            x = tf.nn.relu(x)\n",
    "#             print(x)\n",
    "            \n",
    "            x = t.deconv2d(x, self.channel, 5, 2, name='gen-deconv2d-5')\n",
    "            x = tf.nn.tanh(x)\n",
    "#             print(x)\n",
    "\n",
    "            return x\n",
    "        \n",
    "        \n",
    "    def discriminator(self, x, reuse=None):\n",
    "        \n",
    "        vgg = tf.contrib.keras.applications.VGG16(input_shape=self.IMG_SHAPE, input_tensor=x, weights=\"imagenet\", include_top=False)\n",
    "#             output = tf.identity(vgg.block5_pool, name='my_output')\n",
    "        vgg.trainable = False\n",
    "        output = vgg.layers[-1].output\n",
    "        \n",
    "        with tf.variable_scope('discriminator', reuse=reuse):\n",
    "            \n",
    "#             x = t.conv2d(output, self.df_dim * 1, 5, 2, name='disc-conv2d-1')\n",
    "#             x = tf.nn.leaky_relu(x)\n",
    "\n",
    "#             x = t.conv2d(x, self.df_dim * 2, 5, 2, name='disc-conv2d-2')\n",
    "#             x = t.batch_norm(x, name='disc-bn-1')\n",
    "#             x = tf.nn.leaky_relu(x)\n",
    "\n",
    "#             x = t.conv2d(x, self.df_dim * 4, 5, 2, name='disc-conv2d-3')\n",
    "#             x = t.batch_norm(x, name='disc-bn-2')\n",
    "#             x = tf.nn.leaky_relu(x)\n",
    "\n",
    "#             x = t.conv2d(x, self.df_dim * 8, 5, 2, name='disc-conv2d-4')\n",
    "#             x = t.batch_norm(x, name='disc-bn-3')\n",
    "#             x = tf.nn.leaky_relu(x)\n",
    "\n",
    "            x = tf.layers.flatten(output)\n",
    "\n",
    "            x = t.dense(x, 256, name='disc-fc-1')\n",
    "            x = tf.nn.leaky_relu(x)\n",
    "            \n",
    "            x = t.dense(x, 128, name='disc-fc-2')\n",
    "            x = tf.nn.leaky_relu(x)\n",
    "            \n",
    "            logits = t.dense(x, 1, name='disc-fc-3')\n",
    "#             print(logits)\n",
    "            \n",
    "            prob = tf.nn.sigmoid(logits) # only to check output manually, not given to optimizer\n",
    "            \n",
    "            return prob, logits\n",
    "        \n",
    "        \n",
    "    def build_dcgan(self):\n",
    "        \n",
    "        # Generator\n",
    "        self.g = self.generator(self.z)\n",
    "\n",
    "#         # Discriminator\n",
    "        _, d_real = self.discriminator(self.x)\n",
    "        _, d_fake = self.discriminator(self.g, reuse=True)\n",
    "        \n",
    "        # Losses\n",
    "        d_real_loss = t.sce_loss(d_real, tf.ones_like(d_real))\n",
    "        d_fake_loss = t.sce_loss(d_fake, tf.zeros_like(d_fake))\n",
    "        self.d_loss = d_real_loss + d_fake_loss\n",
    "        self.g_loss = t.sce_loss(d_fake, tf.ones_like(d_fake))\n",
    "        \n",
    "        # Collect trainer values\n",
    "        t_vars = tf.trainable_variables()\n",
    "        d_params = [v for v in t_vars if v.name.startswith('d')]\n",
    "        g_params = [v for v in t_vars if v.name.startswith('g')]\n",
    "\n",
    "        # Optimizer\n",
    "        self.d_op = tf.train.AdamOptimizer(learning_rate=1e-6,\n",
    "                                           beta1=self.beta1).minimize(self.d_loss, var_list=d_params)\n",
    "        self.g_op = tf.train.AdamOptimizer(learning_rate=1e-4,\n",
    "                                           beta1=self.beta1).minimize(self.g_loss, var_list=g_params)\n",
    "        \n",
    "    def train_model(self):\n",
    "        \n",
    "        train = dcgan.extract_data()\n",
    "        dcgan.build_dcgan()\n",
    "\n",
    "        total_epochs = 350\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(train)\n",
    "        dataset = dataset.shuffle(len(train)).repeat().batch(self.batch_size)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        x_data = iterator.get_next()\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        print(\"------------Start of training---------------\")\n",
    "\n",
    "        for epoch in range(1, total_epochs):\n",
    "\n",
    "            for j in range(int(len(train)/self.batch_size)):\n",
    "                x_batch = self.sess.run(x_data)\n",
    "                z_batch = np.random.uniform(-1., 1., [self.batch_size, self.z_dim]).astype(np.float32)\n",
    "                _, dis_loss = self.sess.run([self.d_op, self.d_loss], feed_dict={self.x: x_batch, self.z: z_batch})\n",
    "                _, gen_loss = self.sess.run([self.g_op, self.g_loss], feed_dict={self.x: x_batch, self.z: z_batch})\n",
    "\n",
    "            print(\"Epoch %d, d_loss: %g, g_loss: %g\" %(epoch, dis_loss, gen_loss))\n",
    "\n",
    "            if epoch % 50 == 0:\n",
    "                # Training G model with sample image and noise\n",
    "                sample_z = np.random.uniform(-1., 1., [self.sample_num, self.z_dim])\n",
    "                samples = self.sess.run(self.g, feed_dict={self.z: sample_z})\n",
    "                # Generated image save\n",
    "                for s in range(samples.shape[0]):\n",
    "                    cv.imwrite(\"./airplane_gen/\"+str(epoch)+\"_\"+str(s)+\".jpg\", np.array(samples[s] * 255.0))\n",
    "#                 t.save_images(samples,\n",
    "#                                size=[self.height, self.width],\n",
    "#                                image_path=\"./airplane/\"+,\n",
    "#                                inv_type='127')\n",
    "                \n",
    "        self.sess.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "..........loading dataset from numpy files..........\n",
      "\n",
      "WARNING:tensorflow:From /home/sudhakaran/Desktop/tfutils.py:87: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/sudhakaran/Desktop/3d_recognition_thesis/2d_recognition/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/sudhakaran/Desktop/tfutils.py:98: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From /home/sudhakaran/Desktop/tfutils.py:69: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d_transpose instead.\n",
      "WARNING:tensorflow:From <ipython-input-3-71d87790e08b>:144: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "------------Start of training---------------\n",
      "Epoch 1, d_loss: 1.39525, g_loss: 0.683991\n",
      "Epoch 2, d_loss: 1.39484, g_loss: 0.684308\n",
      "Epoch 3, d_loss: 1.39068, g_loss: 0.688213\n",
      "Epoch 4, d_loss: 1.3865, g_loss: 0.692177\n",
      "Epoch 5, d_loss: 1.38224, g_loss: 0.696085\n",
      "Epoch 6, d_loss: 1.37989, g_loss: 0.697991\n",
      "Epoch 7, d_loss: 1.37786, g_loss: 0.699438\n",
      "Epoch 8, d_loss: 1.37618, g_loss: 0.700552\n",
      "Epoch 9, d_loss: 1.37518, g_loss: 0.700765\n",
      "Epoch 10, d_loss: 1.37423, g_loss: 0.700857\n",
      "Epoch 11, d_loss: 1.37321, g_loss: 0.701029\n",
      "Epoch 12, d_loss: 1.37316, g_loss: 0.700514\n",
      "Epoch 13, d_loss: 1.37285, g_loss: 0.699881\n",
      "Epoch 14, d_loss: 1.37195, g_loss: 0.699892\n",
      "Epoch 15, d_loss: 1.37211, g_loss: 0.698952\n",
      "Epoch 16, d_loss: 1.37196, g_loss: 0.698711\n",
      "Epoch 17, d_loss: 1.37132, g_loss: 0.69851\n",
      "Epoch 18, d_loss: 1.37244, g_loss: 0.697139\n",
      "Epoch 19, d_loss: 1.37254, g_loss: 0.696855\n",
      "Epoch 20, d_loss: 1.37179, g_loss: 0.696537\n",
      "Epoch 21, d_loss: 1.37115, g_loss: 0.696571\n",
      "Epoch 22, d_loss: 1.37021, g_loss: 0.696697\n",
      "Epoch 23, d_loss: 1.36971, g_loss: 0.697145\n",
      "Epoch 24, d_loss: 1.36791, g_loss: 0.697559\n",
      "Epoch 25, d_loss: 1.36726, g_loss: 0.697837\n",
      "Epoch 26, d_loss: 1.36618, g_loss: 0.698499\n",
      "Epoch 27, d_loss: 1.36441, g_loss: 0.69895\n",
      "Epoch 28, d_loss: 1.36316, g_loss: 0.699603\n",
      "Epoch 29, d_loss: 1.36264, g_loss: 0.700018\n",
      "Epoch 30, d_loss: 1.36129, g_loss: 0.700669\n",
      "Epoch 31, d_loss: 1.35966, g_loss: 0.701171\n",
      "Epoch 32, d_loss: 1.35865, g_loss: 0.701699\n",
      "Epoch 33, d_loss: 1.35723, g_loss: 0.702176\n",
      "Epoch 34, d_loss: 1.35593, g_loss: 0.70257\n",
      "Epoch 35, d_loss: 1.35483, g_loss: 0.703424\n",
      "Epoch 36, d_loss: 1.35297, g_loss: 0.704003\n",
      "Epoch 37, d_loss: 1.35228, g_loss: 0.704284\n",
      "Epoch 38, d_loss: 1.35013, g_loss: 0.705256\n",
      "Epoch 39, d_loss: 1.34904, g_loss: 0.705812\n",
      "Epoch 40, d_loss: 1.34675, g_loss: 0.706466\n",
      "Epoch 41, d_loss: 1.34581, g_loss: 0.706964\n",
      "Epoch 42, d_loss: 1.34406, g_loss: 0.707645\n",
      "Epoch 43, d_loss: 1.34259, g_loss: 0.708335\n",
      "Epoch 44, d_loss: 1.34185, g_loss: 0.708979\n",
      "Epoch 45, d_loss: 1.33917, g_loss: 0.709045\n",
      "Epoch 46, d_loss: 1.33829, g_loss: 0.709964\n",
      "Epoch 47, d_loss: 1.33547, g_loss: 0.710492\n",
      "Epoch 48, d_loss: 1.3344, g_loss: 0.710552\n",
      "Epoch 49, d_loss: 1.33242, g_loss: 0.711229\n",
      "Epoch 50, d_loss: 1.33033, g_loss: 0.711746\n",
      "Epoch 51, d_loss: 1.33013, g_loss: 0.712158\n",
      "Epoch 52, d_loss: 1.32819, g_loss: 0.71307\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2e80f214d428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# np_test_path = './test.npy.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# dcgan.discriminator(dcgan.x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-71d87790e08b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0mz_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdis_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mz_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mz_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch %d, d_loss: %g, g_loss: %g\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdis_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/3d_recognition_thesis/2d_recognition/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/3d_recognition_thesis/2d_recognition/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/3d_recognition_thesis/2d_recognition/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/3d_recognition_thesis/2d_recognition/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/3d_recognition_thesis/2d_recognition/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/3d_recognition_thesis/2d_recognition/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dcgan = DCGAN()\n",
    "np_train_path = './train.npy.gz'\n",
    "# np_test_path = './test.npy.gz'\n",
    "# dcgan.discriminator(dcgan.x)\n",
    "dcgan.train_model()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
