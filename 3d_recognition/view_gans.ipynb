{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#To increase cell width of ipynb\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# from collections import defaultdict\n",
    "import gzip\n",
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tfutils as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gf_dim = 64\n",
    "        self.df_dim = 64\n",
    "        self.height = 128\n",
    "        self.width = 128\n",
    "        self.channel = 3\n",
    "        self.z_dim = 128\n",
    "        self.lr = 1e-4\n",
    "        self.beta1 = 0.5\n",
    "        self.sample_num = 5\n",
    "        tf.reset_default_graph()\n",
    "        self.sess = tf.Session()\n",
    "        self.batch_size = 10\n",
    "        # Placeholders\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, self.height, self.width, self.channel], name='x-images')\n",
    "        self.z = tf.placeholder(tf.float32, shape=[None, self.z_dim], name='z-noise')\n",
    "    \n",
    "    def extract_data(self):\n",
    "        train = []\n",
    "        test = []\n",
    "        \n",
    "        if os.path.exists(np_train_path):\n",
    "            \n",
    "            print(\"\\n..........loading dataset from numpy files..........\\n\")\n",
    "            \n",
    "            with gzip.GzipFile(np_train_path, \"r\") as f:\n",
    "                train = np.load(f)\n",
    "        else:\n",
    "            \n",
    "            print(\"\\n..........loading dataset from disk..........\\n\")\n",
    "            \n",
    "            for file in os.listdir(\"./airplane\"):\n",
    "                file_path = os.path.join(\"./airplane\", file)\n",
    "                if str(file) == \"train\":\n",
    "                    for label in sorted(os.listdir(file_path)):\n",
    "                        label_path = os.path.join(file_path, label)\n",
    "                        for img in os.listdir(label_path):\n",
    "                            img_path = os.path.join(label_path, img)\n",
    "                            if str(img[0:-4]) == \"1\":\n",
    "                                train1 = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "                            elif str(img[0:-4]) == \"6\":\n",
    "                                train2 = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "                            elif str(img[0:-4]) == \"10\":\n",
    "                                train3 = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "                        image1 = cv.merge((train1, train2, train3))\n",
    "                        train.append(image1)\n",
    "        #                 cv.imshow(\"image\", image)\n",
    "        #                 cv.waitKey(0)\n",
    "        #                 cv.destroyAllWindows()\n",
    "        #                 cv.imwrite(\"./2.jpg\", image1)\n",
    "        #                 break\n",
    "\n",
    "    #             elif str(file) == \"test\":\n",
    "    #                 for label in sorted(os.listdir(file_path)):\n",
    "    #                     label_path = os.path.join(file_path, label)\n",
    "    #                     for img in os.listdir(label_path):\n",
    "    #                         img_path = os.path.join(label_path, img)\n",
    "    #                         if str(img[0:-4]) == \"1\":\n",
    "    #                             test1 = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "    #                         elif str(img[0:-4]) == \"6\":\n",
    "    #                             test2 = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "    #                         elif str(img[0:-4]) == \"10\":\n",
    "    #                             test3 = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n",
    "    #                     image2 = cv.merge((test1, test2, test3))\n",
    "    #                     test.append(image2)\n",
    "            train = np.array(train, dtype=\"float\") / 255.0\n",
    "    #         test = np.array(test, dtype=\"float\") / 255.0\n",
    "            train = train.reshape(train.shape[0], 128, 128, 3)\n",
    "    #         test = test.reshape(test.shape[0], 128, 128, 3)\n",
    "\n",
    "            with gzip.GzipFile(np_train_path, \"w\") as f:\n",
    "                np.save(f, train)\n",
    "    #         with gzip.GzipFile(np_test_path, \"w\") as f:\n",
    "    #             np.save(f, test)\n",
    "        return train\n",
    "    \n",
    "    # gf_dim * value represents total number of conv filters in that layer, i.e, 64 * 8, 64 * 4, etc.\n",
    "    def generator(self, z, reuse=None, is_train=True):\n",
    "        with tf.variable_scope('generator', reuse=reuse):\n",
    "            x = t.dense(z, self.gf_dim * 8 * 8 * 8, name='gen-fc-1')\n",
    "\n",
    "            x = tf.reshape(x, [-1, 8, 8, self.gf_dim * 8])\n",
    "            x = t.batch_norm(x, is_train=is_train, name='gen-bn-1')\n",
    "            x = tf.nn.relu(x)\n",
    "\n",
    "            x = t.deconv2d(x, self.gf_dim * 4, 5, 2, name='gen-deconv2d-1')\n",
    "            x = t.batch_norm(x, is_train=is_train, name='gen-bn-2')\n",
    "            x = tf.nn.relu(x)\n",
    "\n",
    "            x = t.deconv2d(x,  self.gf_dim * 2, 5, 2, name='gen-deconv2d-2')\n",
    "            x = t.batch_norm(x, is_train=is_train, name='gen-bn-3')\n",
    "            x = tf.nn.relu(x)\n",
    "\n",
    "            x = t.deconv2d(x,  self.gf_dim * 1, 5, 2, name='gen-deconv2d-3')\n",
    "            x = t.batch_norm(x, is_train=is_train, name='gen-bn-4')\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            x = t.deconv2d(x, self.channel, 5, 2, name='gen-deconv2d-4')\n",
    "            x = tf.nn.tanh(x)\n",
    "\n",
    "            return x\n",
    "        \n",
    "        \n",
    "    def discriminator(self, x, reuse=None):\n",
    "        with tf.variable_scope('discriminator', reuse=reuse):\n",
    "            x = t.conv2d(x, self.df_dim * 1, 5, 2, name='disc-conv2d-1')\n",
    "            x = tf.nn.leaky_relu(x)\n",
    "\n",
    "            x = t.conv2d(x, self.df_dim * 2, 5, 2, name='disc-conv2d-2')\n",
    "            x = t.batch_norm(x, name='disc-bn-1')\n",
    "            x = tf.nn.leaky_relu(x)\n",
    "\n",
    "            x = t.conv2d(x, self.df_dim * 4, 5, 2, name='disc-conv2d-3')\n",
    "            x = t.batch_norm(x, name='disc-bn-2')\n",
    "            x = tf.nn.leaky_relu(x)\n",
    "\n",
    "            x = t.conv2d(x, self.df_dim * 8, 5, 2, name='disc-conv2d-4')\n",
    "            x = t.batch_norm(x, name='disc-bn-3')\n",
    "            x = tf.nn.leaky_relu(x)\n",
    "\n",
    "            x = tf.layers.flatten(x)\n",
    "\n",
    "            logits = t.dense(x, 1, name='disc-fc-1')\n",
    "            prob = tf.nn.sigmoid(logits) # only to check output manually, not given to optimizer\n",
    "\n",
    "            return prob, logits\n",
    "        \n",
    "        \n",
    "    def build_dcgan(self):\n",
    "        \n",
    "        # Generator\n",
    "        self.g = self.generator(self.z)\n",
    "\n",
    "        # Discriminator\n",
    "        _, d_real = self.discriminator(self.x)\n",
    "        _, d_fake = self.discriminator(self.g, reuse=True)\n",
    "        \n",
    "        # Losses\n",
    "        d_real_loss = t.sce_loss(d_real, tf.ones_like(d_real))\n",
    "        d_fake_loss = t.sce_loss(d_fake, tf.zeros_like(d_fake))\n",
    "        self.d_loss = d_real_loss + d_fake_loss\n",
    "        self.g_loss = t.sce_loss(d_fake, tf.ones_like(d_fake))\n",
    "        \n",
    "        # Collect trainer values\n",
    "        t_vars = tf.trainable_variables()\n",
    "        d_params = [v for v in t_vars if v.name.startswith('d')]\n",
    "        g_params = [v for v in t_vars if v.name.startswith('g')]\n",
    "\n",
    "        # Optimizer\n",
    "        self.d_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n",
    "                                           beta1=self.beta1).minimize(self.d_loss, var_list=d_params)\n",
    "        self.g_op = tf.train.AdamOptimizer(learning_rate=self.lr,\n",
    "                                           beta1=self.beta1).minimize(self.g_loss, var_list=g_params)\n",
    "        \n",
    "    def train_model(self):\n",
    "        \n",
    "        train = dcgan.extract_data()\n",
    "        dcgan.build_dcgan()\n",
    "\n",
    "        total_epochs = 1000\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(train)\n",
    "        dataset = dataset.shuffle(len(train)).repeat().batch(self.batch_size)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        x_data = iterator.get_next()\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        print(\"------------Start of training---------------\")\n",
    "\n",
    "        for epoch in range(1, total_epochs):\n",
    "\n",
    "            for j in range(int(len(train)/self.batch_size)):\n",
    "                x_batch = self.sess.run(x_data)\n",
    "                z_batch = np.random.uniform(-1., 1., [self.batch_size, self.z_dim]).astype(np.float32)\n",
    "                _, dis_loss = self.sess.run([self.d_op, self.d_loss], feed_dict={self.x: x_batch, self.z: z_batch})\n",
    "                _, gen_loss = self.sess.run([self.g_op, self.g_loss], feed_dict={self.x: x_batch, self.z: z_batch})\n",
    "\n",
    "            print(\"Epoch %d, d_loss: %g, g_loss: %g\" %(epoch, dis_loss, gen_loss))\n",
    "\n",
    "            if epoch % 50 == 0:\n",
    "                # Training G model with sample image and noise\n",
    "                sample_z = np.random.uniform(-1., 1., [self.sample_num, self.z_dim])\n",
    "                samples = self.sess.run(self.g, feed_dict={self.z: sample_z})\n",
    "\n",
    "                # Generated image save\n",
    "                for s in range(samples.shape[0]):\n",
    "                    cv.imwrite(\"./airplane_gen/\"+str(epoch)+\"_\"+str(s)+\".jpg\", np.array(samples[s] * 255.0))\n",
    "#                 t.save_images(samples,\n",
    "#                                size=[self.height, self.width],\n",
    "#                                image_path=\"./airplane/\"+,\n",
    "#                                inv_type='127')\n",
    "                \n",
    "        self.sess.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "..........loading dataset from numpy files..........\n",
      "\n",
      "WARNING:tensorflow:From /home/sudhakaran/Desktop/tfutils.py:87: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/sudhakaran/Desktop/3d_recognition_thesis/2d_recognition/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/sudhakaran/Desktop/tfutils.py:98: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From /home/sudhakaran/Desktop/tfutils.py:69: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d_transpose instead.\n",
      "WARNING:tensorflow:From /home/sudhakaran/Desktop/tfutils.py:47: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-3-b8d962faa87d>:123: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "------------Start of training---------------\n",
      "Epoch 1, d_loss: 0.000848305, g_loss: 11.288\n",
      "Epoch 2, d_loss: 4.09696e-06, g_loss: 13.6469\n",
      "Epoch 3, d_loss: 0.0715537, g_loss: 10.7705\n",
      "Epoch 4, d_loss: 0.100033, g_loss: 5.31447\n",
      "Epoch 5, d_loss: 0.0208974, g_loss: 5.46438\n",
      "Epoch 6, d_loss: 0.0232296, g_loss: 6.66329\n",
      "Epoch 7, d_loss: 2.16171, g_loss: 2.38113\n",
      "Epoch 8, d_loss: 0.0290701, g_loss: 6.75579\n",
      "Epoch 9, d_loss: 0.259035, g_loss: 7.59179\n",
      "Epoch 10, d_loss: 0.0444314, g_loss: 4.1306\n",
      "Epoch 11, d_loss: 0.0498143, g_loss: 5.39937\n",
      "Epoch 12, d_loss: 0.061303, g_loss: 5.19053\n",
      "Epoch 13, d_loss: 0.175166, g_loss: 3.95973\n",
      "Epoch 14, d_loss: 0.04491, g_loss: 7.09156\n",
      "Epoch 15, d_loss: 0.0880776, g_loss: 5.71171\n",
      "Epoch 16, d_loss: 0.0746524, g_loss: 14.5272\n",
      "Epoch 17, d_loss: 0.0198769, g_loss: 6.1713\n",
      "Epoch 18, d_loss: 0.13349, g_loss: 5.68263\n",
      "Epoch 19, d_loss: 0.0960415, g_loss: 3.94377\n",
      "Epoch 20, d_loss: 0.080893, g_loss: 6.08909\n",
      "Epoch 21, d_loss: 0.0214496, g_loss: 10.6845\n",
      "Epoch 22, d_loss: 0.0754506, g_loss: 8.41217\n",
      "Epoch 23, d_loss: 0.058218, g_loss: 7.64582\n",
      "Epoch 24, d_loss: 0.0241418, g_loss: 5.92674\n",
      "Epoch 25, d_loss: 0.0859185, g_loss: 8.83\n",
      "Epoch 26, d_loss: 0.306323, g_loss: 1.60356\n",
      "Epoch 27, d_loss: 0.0127136, g_loss: 7.75038\n",
      "Epoch 28, d_loss: 0.113948, g_loss: 2.84\n",
      "Epoch 29, d_loss: 0.123736, g_loss: 9.39281\n",
      "Epoch 30, d_loss: 0.0919882, g_loss: 5.62871\n",
      "Epoch 31, d_loss: 0.123544, g_loss: 15.1736\n",
      "Epoch 32, d_loss: 0.93905, g_loss: 27.9982\n",
      "Epoch 33, d_loss: 0.202269, g_loss: 9.26145\n",
      "Epoch 34, d_loss: 0.0163778, g_loss: 7.29666\n",
      "Epoch 35, d_loss: 0.10224, g_loss: 5.94524\n",
      "Epoch 36, d_loss: 0.0543938, g_loss: 5.65841\n",
      "Epoch 37, d_loss: 0.0164078, g_loss: 6.92935\n",
      "Epoch 38, d_loss: 0.181933, g_loss: 11.4977\n",
      "Epoch 39, d_loss: 0.00500136, g_loss: 6.94547\n",
      "Epoch 40, d_loss: 0.00052078, g_loss: 8.45898\n",
      "Epoch 41, d_loss: 0.00125763, g_loss: 9.4936\n",
      "Epoch 42, d_loss: 0.0152298, g_loss: 8.63095\n",
      "Epoch 43, d_loss: 0.00429909, g_loss: 9.20308\n",
      "Epoch 44, d_loss: 0.100452, g_loss: 8.28481\n",
      "Epoch 45, d_loss: 0.0212264, g_loss: 5.75427\n",
      "Epoch 46, d_loss: 0.0934826, g_loss: 6.30405\n",
      "Epoch 47, d_loss: 0.0606029, g_loss: 5.2295\n",
      "Epoch 48, d_loss: 0.030705, g_loss: 6.6453\n",
      "Epoch 49, d_loss: 0.00383572, g_loss: 13.2721\n",
      "Epoch 50, d_loss: 0.0302857, g_loss: 7.21414\n",
      "Epoch 51, d_loss: 0.00882893, g_loss: 8.09855\n",
      "Epoch 52, d_loss: 0.000222206, g_loss: 12.0392\n",
      "Epoch 53, d_loss: 0.00117671, g_loss: 6.02914\n",
      "Epoch 54, d_loss: 0.0378537, g_loss: 7.39122\n",
      "Epoch 55, d_loss: 0.0426458, g_loss: 5.51521\n",
      "Epoch 56, d_loss: 0.00194871, g_loss: 6.93509\n",
      "Epoch 57, d_loss: 0.0235337, g_loss: 6.21798\n",
      "Epoch 58, d_loss: 0.0040153, g_loss: 11.973\n",
      "Epoch 59, d_loss: 0.0141465, g_loss: 10.3935\n",
      "Epoch 60, d_loss: 0.0105752, g_loss: 7.26877\n",
      "Epoch 61, d_loss: 0.0684771, g_loss: 19.6204\n",
      "Epoch 62, d_loss: 0.0229423, g_loss: 6.17061\n",
      "Epoch 63, d_loss: 0.101256, g_loss: 6.63749\n",
      "Epoch 64, d_loss: 0.0141458, g_loss: 10.2232\n",
      "Epoch 65, d_loss: 0.131228, g_loss: 7.49201\n",
      "Epoch 66, d_loss: 0.0297874, g_loss: 7.58885\n",
      "Epoch 67, d_loss: 1.90542, g_loss: 0.450745\n",
      "Epoch 68, d_loss: 0.378367, g_loss: 17.3214\n",
      "Epoch 69, d_loss: 0.0874054, g_loss: 6.81474\n",
      "Epoch 70, d_loss: 0.104097, g_loss: 6.21751\n",
      "Epoch 71, d_loss: 0.0388824, g_loss: 5.60013\n",
      "Epoch 72, d_loss: 0.0389584, g_loss: 17.7572\n",
      "Epoch 73, d_loss: 0.00257938, g_loss: 8.08197\n",
      "Epoch 74, d_loss: 0.159166, g_loss: 15.3007\n",
      "Epoch 75, d_loss: 0.042751, g_loss: 5.97054\n",
      "Epoch 76, d_loss: 0.0589056, g_loss: 4.34045\n",
      "Epoch 77, d_loss: 0.0407198, g_loss: 6.66838\n",
      "Epoch 78, d_loss: 0.0115798, g_loss: 5.60633\n",
      "Epoch 79, d_loss: 0.00751779, g_loss: 6.02078\n",
      "Epoch 80, d_loss: 0.00251229, g_loss: 7.86581\n",
      "Epoch 81, d_loss: 0.0213649, g_loss: 5.91332\n",
      "Epoch 82, d_loss: 1.55872, g_loss: 26.2979\n",
      "Epoch 83, d_loss: 0.0782925, g_loss: 6.39961\n",
      "Epoch 84, d_loss: 0.190312, g_loss: 12.2886\n",
      "Epoch 85, d_loss: 0.00855244, g_loss: 6.05649\n",
      "Epoch 86, d_loss: 0.00111732, g_loss: 7.26522\n",
      "Epoch 87, d_loss: 0.00265857, g_loss: 9.98603\n",
      "Epoch 88, d_loss: 0.0137183, g_loss: 7.05279\n",
      "Epoch 89, d_loss: 0.0026419, g_loss: 8.76674\n",
      "Epoch 90, d_loss: 0.0769897, g_loss: 8.29998\n",
      "Epoch 91, d_loss: 0.00088675, g_loss: 9.45607\n",
      "Epoch 92, d_loss: 0.00193184, g_loss: 7.73353\n",
      "Epoch 93, d_loss: 0.00208244, g_loss: 15.5468\n",
      "Epoch 94, d_loss: 0.0549424, g_loss: 5.55389\n",
      "Epoch 95, d_loss: 0.000934602, g_loss: 8.69515\n",
      "Epoch 96, d_loss: 0.00594487, g_loss: 8.05347\n",
      "Epoch 97, d_loss: 0.0397641, g_loss: 13.9257\n",
      "Epoch 98, d_loss: 0.0161791, g_loss: 11.3957\n",
      "Epoch 99, d_loss: 0.0368689, g_loss: 5.85068\n",
      "Epoch 100, d_loss: 0.00444177, g_loss: 5.80272\n",
      "Epoch 101, d_loss: 0.213977, g_loss: 6.03448\n",
      "Epoch 102, d_loss: 0.0424028, g_loss: 7.6103\n",
      "Epoch 103, d_loss: 0.0254223, g_loss: 5.59815\n",
      "Epoch 104, d_loss: 0.217575, g_loss: 28.2058\n",
      "Epoch 105, d_loss: 0.263295, g_loss: 16.0103\n",
      "Epoch 106, d_loss: 0.00910544, g_loss: 17.6202\n",
      "Epoch 107, d_loss: 0.495976, g_loss: 34.2079\n",
      "Epoch 108, d_loss: 0.000596985, g_loss: 7.77131\n",
      "Epoch 109, d_loss: 0.100903, g_loss: 16.7793\n",
      "Epoch 110, d_loss: 0.00653726, g_loss: 21.7652\n",
      "Epoch 111, d_loss: 0.0161516, g_loss: 6.2245\n",
      "Epoch 112, d_loss: 1.41971, g_loss: 28.9904\n",
      "Epoch 113, d_loss: 0.0154848, g_loss: 10.3804\n",
      "Epoch 114, d_loss: 0.00106537, g_loss: 8.00516\n",
      "Epoch 115, d_loss: 0.000467021, g_loss: 10.5599\n",
      "Epoch 116, d_loss: 2.20734, g_loss: 30.9605\n",
      "Epoch 117, d_loss: 0.0705127, g_loss: 5.96681\n",
      "Epoch 118, d_loss: 0.0193501, g_loss: 7.99662\n",
      "Epoch 119, d_loss: 0.0295724, g_loss: 11.5537\n",
      "Epoch 120, d_loss: 0.000871568, g_loss: 8.90197\n",
      "Epoch 121, d_loss: 0.033344, g_loss: 8.11701\n",
      "Epoch 122, d_loss: 0.0659233, g_loss: 7.5353\n",
      "Epoch 123, d_loss: 2.26946, g_loss: 15.3984\n",
      "Epoch 124, d_loss: 0.00938283, g_loss: 6.32911\n",
      "Epoch 125, d_loss: 0.00077557, g_loss: 9.41129\n",
      "Epoch 126, d_loss: 0.00868026, g_loss: 12.6985\n",
      "Epoch 127, d_loss: 0.00080105, g_loss: 9.23148\n",
      "Epoch 128, d_loss: 0.00101499, g_loss: 7.54998\n",
      "Epoch 129, d_loss: 0.0139244, g_loss: 15.0814\n",
      "Epoch 130, d_loss: 0.000237108, g_loss: 8.66355\n",
      "Epoch 131, d_loss: 0.282597, g_loss: 15.5106\n",
      "Epoch 132, d_loss: 5.69259e-05, g_loss: 14.4232\n",
      "Epoch 133, d_loss: 0.345389, g_loss: 4.01089\n",
      "Epoch 134, d_loss: 0.000682001, g_loss: 12.1453\n",
      "Epoch 135, d_loss: 0.0025292, g_loss: 7.09769\n",
      "Epoch 136, d_loss: 0.0723447, g_loss: 7.23744\n",
      "Epoch 137, d_loss: 0.000336108, g_loss: 19.9375\n",
      "Epoch 138, d_loss: 0.0555627, g_loss: 5.65105\n",
      "Epoch 139, d_loss: 0.151698, g_loss: 8.71877\n",
      "Epoch 140, d_loss: 0.000848096, g_loss: 7.46578\n",
      "Epoch 141, d_loss: 0.0578011, g_loss: 7.53815\n",
      "Epoch 142, d_loss: 0.0349402, g_loss: 9.20685\n",
      "Epoch 143, d_loss: 0.0088897, g_loss: 6.82413\n",
      "Epoch 144, d_loss: 0.00512226, g_loss: 7.56919\n",
      "Epoch 145, d_loss: 0.305804, g_loss: 8.45604\n",
      "Epoch 146, d_loss: 0.0142077, g_loss: 11.2381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147, d_loss: 0.0364141, g_loss: 6.37698\n",
      "Epoch 148, d_loss: 0.00170738, g_loss: 6.72216\n",
      "Epoch 149, d_loss: 0.700753, g_loss: 13.2209\n",
      "Epoch 150, d_loss: 0.00208281, g_loss: 6.82552\n",
      "Epoch 151, d_loss: 0.00471064, g_loss: 8.20988\n",
      "Epoch 152, d_loss: 0.00628668, g_loss: 15.0649\n",
      "Epoch 153, d_loss: 0.000413069, g_loss: 12.8845\n",
      "Epoch 154, d_loss: 0.0200849, g_loss: 7.63263\n",
      "Epoch 155, d_loss: 0.000461343, g_loss: 8.67807\n",
      "Epoch 156, d_loss: 0.400251, g_loss: 14.6832\n",
      "Epoch 157, d_loss: 0.000970133, g_loss: 9.50993\n",
      "Epoch 158, d_loss: 0.00412795, g_loss: 7.41738\n",
      "Epoch 159, d_loss: 0.00353174, g_loss: 10.3851\n",
      "Epoch 160, d_loss: 0.0223698, g_loss: 9.61844\n",
      "Epoch 161, d_loss: 0.00314129, g_loss: 7.56286\n",
      "Epoch 162, d_loss: 0.124318, g_loss: 13.709\n",
      "Epoch 163, d_loss: 0.00984798, g_loss: 5.61721\n",
      "Epoch 164, d_loss: 0.0294402, g_loss: 27.6008\n",
      "Epoch 165, d_loss: 8.36769e-05, g_loss: 9.79173\n",
      "Epoch 166, d_loss: 0.0026139, g_loss: 7.15774\n",
      "Epoch 167, d_loss: 0.0112309, g_loss: 5.38753\n",
      "Epoch 168, d_loss: 0.229568, g_loss: 13.3723\n",
      "Epoch 169, d_loss: 0.00432405, g_loss: 13.0312\n",
      "Epoch 170, d_loss: 0.00705433, g_loss: 5.72126\n",
      "Epoch 171, d_loss: 0.00731818, g_loss: 6.09389\n",
      "Epoch 172, d_loss: 0.0260704, g_loss: 17.8662\n",
      "Epoch 173, d_loss: 0.000475717, g_loss: 8.538\n",
      "Epoch 174, d_loss: 0.00637148, g_loss: 6.77223\n",
      "Epoch 175, d_loss: 0.0104298, g_loss: 8.90024\n",
      "Epoch 176, d_loss: 0.49075, g_loss: 13.4296\n",
      "Epoch 177, d_loss: 8.22532e-05, g_loss: 14.951\n",
      "Epoch 178, d_loss: 0.0478549, g_loss: 5.321\n",
      "Epoch 179, d_loss: 4.32395e-05, g_loss: 10.6485\n",
      "Epoch 180, d_loss: 0.0103016, g_loss: 8.43972\n",
      "Epoch 181, d_loss: 0.00079706, g_loss: 8.04222\n",
      "Epoch 182, d_loss: 0.000101095, g_loss: 13.4202\n",
      "Epoch 183, d_loss: 0.101706, g_loss: 6.5342\n",
      "Epoch 184, d_loss: 0.0036501, g_loss: 26.6833\n",
      "Epoch 185, d_loss: 0.00126841, g_loss: 24.9443\n",
      "Epoch 186, d_loss: 0.00610193, g_loss: 6.63974\n",
      "Epoch 187, d_loss: 0.313317, g_loss: 13.1451\n",
      "Epoch 188, d_loss: 0.000804343, g_loss: 7.65812\n",
      "Epoch 189, d_loss: 0.000721546, g_loss: 17.3549\n",
      "Epoch 190, d_loss: 0.00326545, g_loss: 7.05798\n",
      "Epoch 191, d_loss: 0.0296197, g_loss: 5.31842\n",
      "Epoch 192, d_loss: 0.00586172, g_loss: 7.16167\n",
      "Epoch 193, d_loss: 0.0204867, g_loss: 16.4385\n",
      "Epoch 194, d_loss: 0.00510693, g_loss: 5.85285\n",
      "Epoch 195, d_loss: 0.0209986, g_loss: 11.1833\n",
      "Epoch 196, d_loss: 0.00747332, g_loss: 6.46911\n",
      "Epoch 197, d_loss: 0.0410178, g_loss: 6.30046\n",
      "Epoch 198, d_loss: 0.0354667, g_loss: 4.90522\n",
      "Epoch 199, d_loss: 0.00167066, g_loss: 7.51803\n",
      "Epoch 200, d_loss: 0.00445793, g_loss: 6.74723\n",
      "Epoch 201, d_loss: 0.00368145, g_loss: 8.02528\n",
      "Epoch 202, d_loss: 0.010912, g_loss: 5.81517\n",
      "Epoch 203, d_loss: 0.00108734, g_loss: 9.73182\n",
      "Epoch 204, d_loss: 0.0216843, g_loss: 5.08257\n",
      "Epoch 205, d_loss: 0.00560459, g_loss: 6.29401\n",
      "Epoch 206, d_loss: 0.0057725, g_loss: 6.99288\n",
      "Epoch 207, d_loss: 0.0829357, g_loss: 7.46876\n",
      "Epoch 208, d_loss: 0.0240643, g_loss: 9.94062\n",
      "Epoch 209, d_loss: 0.102349, g_loss: 7.52951\n",
      "Epoch 210, d_loss: 0.0173729, g_loss: 6.52074\n",
      "Epoch 211, d_loss: 0.021997, g_loss: 5.90935\n",
      "Epoch 212, d_loss: 0.000232795, g_loss: 12.2977\n",
      "Epoch 213, d_loss: 0.116809, g_loss: 8.4753\n",
      "Epoch 214, d_loss: 0.0183157, g_loss: 5.6696\n",
      "Epoch 215, d_loss: 0.0311736, g_loss: 8.23377\n",
      "Epoch 216, d_loss: 0.0657139, g_loss: 4.47435\n",
      "Epoch 217, d_loss: 0.150186, g_loss: 5.04936\n",
      "Epoch 218, d_loss: 0.0178705, g_loss: 5.55412\n",
      "Epoch 219, d_loss: 0.389776, g_loss: 18.1916\n",
      "Epoch 220, d_loss: 0.661488, g_loss: 15.8255\n",
      "Epoch 221, d_loss: 0.0589244, g_loss: 8.53539\n",
      "Epoch 222, d_loss: 0.34922, g_loss: 3.81659\n",
      "Epoch 223, d_loss: 0.102632, g_loss: 6.71537\n",
      "Epoch 224, d_loss: 0.164484, g_loss: 15.2119\n",
      "Epoch 225, d_loss: 0.248677, g_loss: 14.3007\n",
      "Epoch 226, d_loss: 0.0133509, g_loss: 16.9869\n",
      "Epoch 227, d_loss: 0.385042, g_loss: 2.70696\n",
      "Epoch 228, d_loss: 0.00273574, g_loss: 10.305\n",
      "Epoch 229, d_loss: 0.000112751, g_loss: 9.56007\n",
      "Epoch 230, d_loss: 0.0164816, g_loss: 17.661\n",
      "Epoch 231, d_loss: 0.0082183, g_loss: 6.19714\n",
      "Epoch 232, d_loss: 0.00252589, g_loss: 9.33055\n",
      "Epoch 233, d_loss: 0.0128357, g_loss: 6.39759\n",
      "Epoch 234, d_loss: 0.00332038, g_loss: 8.43122\n",
      "Epoch 235, d_loss: 3.30116e-05, g_loss: 14.1325\n",
      "Epoch 236, d_loss: 0.0473017, g_loss: 6.80244\n",
      "Epoch 237, d_loss: 0.1198, g_loss: 6.01843\n",
      "Epoch 238, d_loss: 0.000438823, g_loss: 11.3335\n",
      "Epoch 239, d_loss: 0.0865359, g_loss: 7.77207\n",
      "Epoch 240, d_loss: 0.145398, g_loss: 3.64861\n",
      "Epoch 241, d_loss: 0.114204, g_loss: 5.35308\n",
      "Epoch 242, d_loss: 0.000809095, g_loss: 8.20132\n",
      "Epoch 243, d_loss: 0.233432, g_loss: 13.9619\n",
      "Epoch 244, d_loss: 0.0721821, g_loss: 8.03258\n",
      "Epoch 245, d_loss: 0.0113086, g_loss: 6.25782\n",
      "Epoch 246, d_loss: 1.10463, g_loss: 22.8787\n",
      "Epoch 247, d_loss: 0.246084, g_loss: 9.03151\n",
      "Epoch 248, d_loss: 0.0180351, g_loss: 14.329\n",
      "Epoch 249, d_loss: 0.00251108, g_loss: 6.65688\n",
      "Epoch 250, d_loss: 5.91667e-06, g_loss: 13.5793\n",
      "Epoch 251, d_loss: 0.0146282, g_loss: 12.5316\n",
      "Epoch 252, d_loss: 0.00262253, g_loss: 6.40813\n",
      "Epoch 253, d_loss: 0.00342172, g_loss: 6.59689\n",
      "Epoch 254, d_loss: 0.000234452, g_loss: 12.714\n",
      "Epoch 255, d_loss: 0.00460322, g_loss: 8.46417\n",
      "Epoch 256, d_loss: 0.00362852, g_loss: 7.1171\n",
      "Epoch 257, d_loss: 0.00782964, g_loss: 7.37247\n",
      "Epoch 258, d_loss: 0.0261819, g_loss: 7.28557\n",
      "Epoch 259, d_loss: 0.000588025, g_loss: 8.22282\n",
      "Epoch 260, d_loss: 0.00205186, g_loss: 8.99205\n",
      "Epoch 261, d_loss: 0.0225221, g_loss: 6.12309\n",
      "Epoch 262, d_loss: 0.115669, g_loss: 4.1141\n",
      "Epoch 263, d_loss: 0.00515564, g_loss: 6.11643\n",
      "Epoch 264, d_loss: 0.0206128, g_loss: 5.10396\n",
      "Epoch 265, d_loss: 0.0028579, g_loss: 8.00011\n",
      "Epoch 266, d_loss: 3.62605e-05, g_loss: 13.38\n",
      "Epoch 267, d_loss: 4.56405e-06, g_loss: 21.205\n",
      "Epoch 268, d_loss: 0.0582202, g_loss: 7.78979\n",
      "Epoch 269, d_loss: 0.0701997, g_loss: 7.06897\n",
      "Epoch 270, d_loss: 0.000902013, g_loss: 7.872\n",
      "Epoch 271, d_loss: 0.00458227, g_loss: 5.80161\n",
      "Epoch 272, d_loss: 0.00482194, g_loss: 6.47157\n",
      "Epoch 273, d_loss: 0.0305178, g_loss: 5.63607\n",
      "Epoch 274, d_loss: 1.64452, g_loss: 24.995\n",
      "Epoch 275, d_loss: 0.00530736, g_loss: 15.8491\n",
      "Epoch 276, d_loss: 0.0309608, g_loss: 7.00835\n",
      "Epoch 277, d_loss: 0.00605289, g_loss: 6.89255\n",
      "Epoch 278, d_loss: 2.86542e-05, g_loss: 13.6107\n",
      "Epoch 279, d_loss: 0.00546704, g_loss: 24.2994\n",
      "Epoch 280, d_loss: 0.515811, g_loss: 6.02984\n",
      "Epoch 281, d_loss: 0.00289534, g_loss: 8.83338\n",
      "Epoch 282, d_loss: 0.442685, g_loss: 18.7087\n",
      "Epoch 283, d_loss: 0.192807, g_loss: 17.6372\n",
      "Epoch 284, d_loss: 0.0715068, g_loss: 8.37973\n",
      "Epoch 285, d_loss: 0.079097, g_loss: 31.85\n",
      "Epoch 286, d_loss: 0.00324179, g_loss: 6.34511\n",
      "Epoch 287, d_loss: 0.018594, g_loss: 5.89541\n",
      "Epoch 288, d_loss: 0.00134062, g_loss: 14.5486\n",
      "Epoch 289, d_loss: 0.0700346, g_loss: 14.2411\n",
      "Epoch 290, d_loss: 0.0360081, g_loss: 5.75358\n",
      "Epoch 291, d_loss: 0.0334255, g_loss: 5.99514\n",
      "Epoch 292, d_loss: 0.000982426, g_loss: 8.29368\n",
      "Epoch 293, d_loss: 0.0404511, g_loss: 7.67379\n",
      "Epoch 294, d_loss: 0.000502616, g_loss: 18.1684\n",
      "Epoch 295, d_loss: 8.29716e-05, g_loss: 20.3643\n",
      "Epoch 296, d_loss: 0.00180442, g_loss: 8.47375\n",
      "Epoch 297, d_loss: 0.035253, g_loss: 13.0246\n",
      "Epoch 298, d_loss: 0.152193, g_loss: 7.69237\n",
      "Epoch 299, d_loss: 0.00440554, g_loss: 9.98916\n",
      "Epoch 300, d_loss: 0.0636765, g_loss: 8.01745\n",
      "Epoch 301, d_loss: 0.0186051, g_loss: 5.81103\n",
      "Epoch 302, d_loss: 0.00136679, g_loss: 8.784\n",
      "Epoch 303, d_loss: 0.00768272, g_loss: 7.74858\n",
      "Epoch 304, d_loss: 0.0783231, g_loss: 8.411\n",
      "Epoch 305, d_loss: 0.00092383, g_loss: 11.3242\n",
      "Epoch 306, d_loss: 0.00185046, g_loss: 7.30675\n",
      "Epoch 307, d_loss: 0.000596337, g_loss: 8.31315\n",
      "Epoch 308, d_loss: 5.14394e-05, g_loss: 15.7336\n",
      "Epoch 309, d_loss: 0.163383, g_loss: 34.1791\n",
      "Epoch 310, d_loss: 0.0128146, g_loss: 6.18409\n",
      "Epoch 311, d_loss: 0.0116737, g_loss: 5.99677\n",
      "Epoch 312, d_loss: 0.00280282, g_loss: 9.43382\n",
      "Epoch 313, d_loss: 0.00205489, g_loss: 7.90211\n",
      "Epoch 314, d_loss: 0.00228732, g_loss: 27.9456\n",
      "Epoch 315, d_loss: 0.00550816, g_loss: 6.4212\n",
      "Epoch 316, d_loss: 0.0339635, g_loss: 15.1321\n",
      "Epoch 317, d_loss: 0.0101708, g_loss: 9.49332\n",
      "Epoch 318, d_loss: 0.00925265, g_loss: 7.61038\n",
      "Epoch 319, d_loss: 0.00960569, g_loss: 21.6983\n",
      "Epoch 320, d_loss: 0.0019355, g_loss: 9.62523\n",
      "Epoch 321, d_loss: 0.311629, g_loss: 15.3421\n",
      "Epoch 322, d_loss: 0.0301562, g_loss: 5.90958\n",
      "Epoch 323, d_loss: 0.00222648, g_loss: 8.20855\n",
      "Epoch 324, d_loss: 0.000365948, g_loss: 13.5094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 325, d_loss: 0.000115672, g_loss: 20.4552\n",
      "Epoch 326, d_loss: 0.0694726, g_loss: 6.43868\n",
      "Epoch 327, d_loss: 0.00210715, g_loss: 6.52014\n",
      "Epoch 328, d_loss: 0.265244, g_loss: 16.3917\n",
      "Epoch 329, d_loss: 0.00272047, g_loss: 7.84013\n",
      "Epoch 330, d_loss: 0.00933011, g_loss: 6.16972\n",
      "Epoch 331, d_loss: 0.155251, g_loss: 13.8503\n",
      "Epoch 332, d_loss: 0.001286, g_loss: 11.2583\n",
      "Epoch 333, d_loss: 0.0739664, g_loss: 7.86189\n",
      "Epoch 334, d_loss: 0.00049791, g_loss: 10.2096\n",
      "Epoch 335, d_loss: 0.000147693, g_loss: 12.1449\n",
      "Epoch 336, d_loss: 0.000226517, g_loss: 11.8296\n",
      "Epoch 337, d_loss: 0.000409175, g_loss: 14.1994\n",
      "Epoch 338, d_loss: 0.0107709, g_loss: 6.67535\n",
      "Epoch 339, d_loss: 0.00327115, g_loss: 13.2803\n",
      "Epoch 340, d_loss: 0.000119156, g_loss: 12.0719\n",
      "Epoch 341, d_loss: 0.0285842, g_loss: 7.94023\n",
      "Epoch 342, d_loss: 0.00547334, g_loss: 6.12344\n",
      "Epoch 343, d_loss: 0.00365327, g_loss: 7.14924\n",
      "Epoch 344, d_loss: 0.100491, g_loss: 7.53346\n",
      "Epoch 345, d_loss: 0.0269095, g_loss: 5.49475\n",
      "Epoch 346, d_loss: 3.56166e-05, g_loss: 12.8464\n",
      "Epoch 347, d_loss: 0.00226565, g_loss: 6.9212\n",
      "Epoch 348, d_loss: 0.0483562, g_loss: 6.41139\n",
      "Epoch 349, d_loss: 0.400432, g_loss: 4.62516\n",
      "Epoch 350, d_loss: 0.699191, g_loss: 13.9445\n",
      "Epoch 351, d_loss: 0.00588346, g_loss: 9.77587\n",
      "Epoch 352, d_loss: 0.0411181, g_loss: 7.2317\n",
      "Epoch 353, d_loss: 0.00885484, g_loss: 6.07896\n",
      "Epoch 354, d_loss: 0.0337835, g_loss: 11.6408\n",
      "Epoch 355, d_loss: 0.0100604, g_loss: 5.61865\n",
      "Epoch 356, d_loss: 0.00234857, g_loss: 8.16069\n",
      "Epoch 357, d_loss: 0.00503569, g_loss: 10.0666\n",
      "Epoch 358, d_loss: 0.374363, g_loss: 36.7745\n",
      "Epoch 359, d_loss: 0.000705565, g_loss: 10.5426\n",
      "Epoch 360, d_loss: 0.422689, g_loss: 15.7027\n",
      "Epoch 361, d_loss: 0.0433578, g_loss: 5.37174\n",
      "Epoch 362, d_loss: 0.00180027, g_loss: 12.2868\n",
      "Epoch 363, d_loss: 0.0007296, g_loss: 8.78567\n",
      "Epoch 364, d_loss: 0.0076336, g_loss: 9.73674\n",
      "Epoch 365, d_loss: 0.00459731, g_loss: 7.51353\n",
      "Epoch 366, d_loss: 0.00288392, g_loss: 6.87433\n",
      "Epoch 367, d_loss: 0.0162377, g_loss: 6.73567\n",
      "Epoch 368, d_loss: 0.00707025, g_loss: 6.43913\n",
      "Epoch 369, d_loss: 0.0044444, g_loss: 6.69297\n",
      "Epoch 370, d_loss: 6.51831e-05, g_loss: 10.339\n",
      "Epoch 371, d_loss: 0.00036196, g_loss: 8.63129\n",
      "Epoch 372, d_loss: 0.0257164, g_loss: 7.45097\n",
      "Epoch 373, d_loss: 0.00366263, g_loss: 6.50174\n",
      "Epoch 374, d_loss: 0.00101039, g_loss: 7.32531\n",
      "Epoch 375, d_loss: 0.00140824, g_loss: 12.6296\n",
      "Epoch 376, d_loss: 0.00587674, g_loss: 6.72766\n",
      "Epoch 377, d_loss: 0.019966, g_loss: 5.71368\n",
      "Epoch 378, d_loss: 0.00540666, g_loss: 10.0079\n",
      "Epoch 379, d_loss: 8.36591e-05, g_loss: 19.1518\n",
      "Epoch 380, d_loss: 0.0377361, g_loss: 5.65809\n",
      "Epoch 381, d_loss: 0.00533029, g_loss: 6.38906\n",
      "Epoch 382, d_loss: 0.0253091, g_loss: 5.78623\n",
      "Epoch 383, d_loss: 0.000347018, g_loss: 9.59804\n",
      "Epoch 384, d_loss: 0.000663062, g_loss: 12.5032\n",
      "Epoch 385, d_loss: 0.0190085, g_loss: 6.00655\n",
      "Epoch 386, d_loss: 0.000407909, g_loss: 8.45436\n",
      "Epoch 387, d_loss: 0.0440334, g_loss: 7.13665\n",
      "Epoch 388, d_loss: 0.00560256, g_loss: 7.49507\n",
      "Epoch 389, d_loss: 0.000136804, g_loss: 8.82893\n",
      "Epoch 390, d_loss: 0.245839, g_loss: 5.361\n",
      "Epoch 391, d_loss: 0.0142526, g_loss: 7.26031\n",
      "Epoch 392, d_loss: 0.00177548, g_loss: 7.23691\n",
      "Epoch 393, d_loss: 0.114762, g_loss: 7.99883\n",
      "Epoch 394, d_loss: 0.00377531, g_loss: 11.3393\n",
      "Epoch 395, d_loss: 0.000258588, g_loss: 15.6607\n",
      "Epoch 396, d_loss: 0.0401248, g_loss: 11.8849\n",
      "Epoch 397, d_loss: 2.92671e-06, g_loss: 19.437\n",
      "Epoch 398, d_loss: 1.25415e-05, g_loss: 17.7045\n",
      "Epoch 399, d_loss: 0.00282738, g_loss: 9.89857\n",
      "Epoch 400, d_loss: 0.000694647, g_loss: 9.88945\n",
      "Epoch 401, d_loss: 0.00707918, g_loss: 6.54442\n",
      "Epoch 402, d_loss: 0.0429857, g_loss: 7.62579\n",
      "Epoch 403, d_loss: 0.0527977, g_loss: 10.4939\n",
      "Epoch 404, d_loss: 0.000718333, g_loss: 12.994\n"
     ]
    }
   ],
   "source": [
    "dcgan = DCGAN()\n",
    "np_train_path = './train.npy.gz'\n",
    "# np_test_path = './test.npy.gz'\n",
    "\n",
    "dcgan.train_model()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
