{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEN():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.last_label_index = 0    # tracks the label index upto which the model has been trained\n",
    "        self.params = dict()\n",
    "        self.k_ex = 3\n",
    "        self.den_layers = 3\n",
    "        self.conv_layers = 2\n",
    "        tf.reset_default_graph()\n",
    "        self.sess = None\n",
    "        self.train = []\n",
    "        self.train_labels = []\n",
    "        self.test = []\n",
    "        self.test_labels = []\n",
    "        self.selected = dict()\n",
    "        self.l2_mu = 0.01\n",
    "        self.lamba_regular = 0.5\n",
    "        self.l1_thr = 0.00001\n",
    "        self.loss_thr = 0.01\n",
    "        self.IMG_SIZE = 100\n",
    "        self.IMG_SHAPE = (self.IMG_SIZE, self.IMG_SIZE, 3)\n",
    "#         self.vgg = tf.keras.applications.VGG16(input_shape=self.IMG_SHAPE, weights=\"imagenet\", include_top=False)\n",
    "\n",
    "    def extract_data(self, filepath):\n",
    "        data = []\n",
    "        data_labels = []\n",
    "\n",
    "        for label in os.listdir(filepath):\n",
    "            label_path = os.path.join(filepath, label)\n",
    "            count = 0\n",
    "            list = len(os.listdir(label_path))\n",
    "            for img in os.listdir(label_path):\n",
    "                image_path = os.path.join(label_path, img)\n",
    "                image = cv.imread(image_path)\n",
    "                re_image = cv.resize(image, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                # grayImage = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "                # (thresh, BW) = cv.threshold(grayImage, 127, 255, cv.THRESH_BINARY)\n",
    "                data.append(re_image)\n",
    "                data_labels.append(label)\n",
    "\n",
    "        data = np.array(data, dtype=\"float\") / 255.0\n",
    "        features = self.vgg.predict(data)\n",
    "        features_flatten = features.reshape((features.shape[0], 3 * 3 * 512))\n",
    "        # data = data.reshape(data.shape[0], 50, 50, 3)\n",
    "        data_labels = np.array(data_labels)\n",
    "        label_names = np.unique(data_labels)\n",
    "        # data_labels = tf.one_hot(indices=data_labels, depth=10)\n",
    "        return features_flatten, data_labels, label_names\n",
    "\n",
    "\n",
    "    def add_task(self, task_id, label_names, initial_output=2):\n",
    "\n",
    "        new_label_indices = []\n",
    "        self.train = []\n",
    "        self.train_labels = []\n",
    "        self.train_ex_labels = []      \n",
    " \n",
    "        if task_id == 1:\n",
    "            self.new_train = []\n",
    "            self.new_train_labels = []\n",
    "            self.new_train_ex_labels = []\n",
    "            self.total = []\n",
    "            self.total_labels = []\n",
    "            self.total_ex_labels = []\n",
    "            self.test = []\n",
    "            self.test_labels = []\n",
    "            for i in range(initial_output):   # By default, first task is a binary classification\n",
    "                new_label_indices.append(i)\n",
    "            self.last_label_index = i\n",
    "\n",
    "        else:\n",
    "            new_label_indices.append(self.last_label_index+1)\n",
    "            self.last_label_index = self.last_label_index + 1\n",
    "            \n",
    "            self.total = self.total + self.new_train\n",
    "            self.total_ex_labels = self.total_labels + self.new_train_ex_labels\n",
    "            self.total_labels = self.total_labels + self.new_train_labels\n",
    "            self.new_train = []\n",
    "            self.new_train_labels = []\n",
    "            self.new_train_ex_labels = []\n",
    "\n",
    "        for index in new_label_indices:\n",
    "            print(\" \\n Added new category: \"+str(label_names[index]))\n",
    "            l = 1 if task_id != 1 else index\n",
    "            for data, label in zip(train_data, train_labels):\n",
    "                if label_names[index] == label:\n",
    "                    self.new_train.append(data)\n",
    "                    self.new_train_labels.append(l)\n",
    "                    self.new_train_ex_labels.append(index)\n",
    "\n",
    "            for data, label in zip(test_data, test_labels):\n",
    "                if label_names[index] == label:\n",
    "                    self.test.append(data)\n",
    "                    self.test_labels.append(index)\n",
    "                    \n",
    "        # ------------------ Random sampling old training data ------------------\n",
    "        if task_id != 1:\n",
    "            sampled_indices = random.sample(range(len(self.total)), len(self.new_train))\n",
    "            for k in sampled_indices:\n",
    "                self.train.append(self.total[k])\n",
    "                self.train_labels.append(0)\n",
    "                self.train_ex_labels.append(self.total_ex_labels[k])\n",
    "                \n",
    "            self.train = self.train + self.new_train\n",
    "            self.train_labels = self.train_labels + self.new_train_labels\n",
    "            self.train_ex_labels = self.train_ex_labels + self.new_train_ex_labels\n",
    "        else:\n",
    "            self.train = self.new_train\n",
    "            self.train_labels = self.new_train_labels\n",
    "            self.train_ex_labels = self.new_train_ex_labels\n",
    "                \n",
    "\n",
    "    def destroy_graph(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.params = dict()\n",
    "\n",
    "    def initialize_parameters(self, output_len=2):\n",
    "        self.sess = tf.Session()\n",
    "        self.x = tf.placeholder(tf.float32, [None, 3 * 3 * 512])\n",
    "        self.y_ = tf.placeholder(tf.float32, [None, output_len])\n",
    "        # self.keep_prob = tf.placeholder(tf.float32)          # dropout probability\n",
    "\n",
    "    def create_variable(self, name=None, shape=None, scope=None, trainable=True):\n",
    "        with tf.variable_scope(scope, reuse=False):\n",
    "            w = tf.get_variable(name, shape=shape, \n",
    "#                                 initializer=tf.random_normal_initializer(mean=0, stddev=1, seed=3),\n",
    "                                trainable=trainable)\n",
    "            if \"ex\" not in name:\n",
    "                self.params[w.name] = w\n",
    "        return w\n",
    "\n",
    "    def get_variable(self, name=None, scope=None):\n",
    "        with tf.variable_scope(scope, reuse=True):\n",
    "            w = tf.get_variable(name)\n",
    "            if \"ex\" not in name:\n",
    "                self.params[w.name] = w\n",
    "        return w\n",
    "\n",
    "    def restore_params(self, task_id=None, trainable=True, param_values=dict()):\n",
    "        self.params = dict()\n",
    "        self.prev_W = dict()\n",
    "        for scope_name, value in param_values.items():\n",
    "            self.prev_W[scope_name] = value\n",
    "            scope_name = scope_name.split(':')[0]\n",
    "            [scope, name] = scope_name.split('/')\n",
    "\n",
    "            if task_id != None:\n",
    "                if 'l%d/w_%d' % (self.den_layers,task_id) in scope_name:\n",
    "                    trainable = True\n",
    "                else:\n",
    "                    trainable = False\n",
    "\n",
    "            with tf.variable_scope(scope, reuse=False):\n",
    "                w = tf.get_variable(name, initializer=value, trainable=trainable)\n",
    "            self.params[w.name] = w\n",
    "\n",
    "    def get_params(self):\n",
    "        vdict = dict()\n",
    "        for scope_name, ref_w in self.params.items():\n",
    "            vdict[scope_name] = self.sess.run(ref_w)\n",
    "        return vdict\n",
    "\n",
    "    def conv2d(self, x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    def max_pool_2x2(self, x):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "    def build_model(self, task_id, expansion=False, output_len=2):\n",
    "\n",
    "        # Note: scope and name values are only given to DEN layers, not for fixed sized layers.\n",
    "\n",
    "        self.initialize_parameters(output_len)\n",
    "\n",
    "        if task_id == 1:\n",
    "\n",
    "#             #First Convolutional layers\n",
    "#             W_conv1 = self.create_variable(name=\"w\",shape=[5, 5, 3, 32],scope=\"conv1\")\n",
    "#             b_conv1 = self.create_variable(name=\"b\",shape=[32],scope=\"conv1\")\n",
    "\n",
    "#             h_conv1 = tf.nn.relu(self.conv2d(self.x, W_conv1) + b_conv1)\n",
    "#             h_pool1 = self.max_pool_2x2(h_conv1)\n",
    "\n",
    "#             #Second Convolutional Layer\n",
    "#             W_conv2 = self.create_variable(name=\"w\",shape=[5, 5, 32, 64],scope=\"conv2\")\n",
    "#             b_conv2 = self.create_variable(name=\"b\",shape=[64],scope=\"conv2\")\n",
    "\n",
    "#             h_conv2 = tf.nn.relu(self.conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "#             h_pool2 = self.max_pool_2x2(h_conv2)\n",
    "#             self.h_pool2_flat = tf.reshape(h_pool2, [-1, 15*15*64])\n",
    "\n",
    "            #flattened first fc layer\n",
    "            W_fc1 = self.create_variable(name=\"w\", shape=[3 * 3 * 512, 1024], scope=\"l1\")   # layer-1 outgoing weight matrix\n",
    "            b_fc1 = self.create_variable(name=\"b\", shape=[1024], scope=\"l1\")\n",
    "\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(self.x, W_fc1) + b_fc1)\n",
    "\n",
    "            #second fc layer\n",
    "            W_fc2 = self.create_variable(name=\"w\", shape=[1024, 128], scope=\"l2\")   # layer-2 outgoing weight matrix\n",
    "            b_fc2 = self.create_variable(name=\"b\", shape=[128], scope=\"l2\")\n",
    "\n",
    "            self.h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "\n",
    "            # readout fc layer\n",
    "            self.w_fc = self.create_variable(name=\"w\", shape=[128, output_len], scope=\"l3\")     # layer-3 outgoing weight matrix \n",
    "            self.b_fc = self.create_variable(name=\"b\", shape=[output_len], scope=\"l3\")\n",
    "            y_conv = tf.matmul(self.h_fc2, self.w_fc) + self.b_fc\n",
    "            \n",
    "        elif expansion:\n",
    "            \n",
    "            #First Convolutional layers\n",
    "#             W_conv1 = self.get_variable(name=\"w\",scope=\"conv1\")\n",
    "#             b_conv1 = self.get_variable(name=\"b\",scope=\"conv1\")\n",
    "\n",
    "#             h_conv1 = tf.nn.relu(self.conv2d(self.x, W_conv1) + b_conv1)\n",
    "#             h_pool1 = self.max_pool_2x2(h_conv1)\n",
    "\n",
    "#             #Second Convolutional Layer\n",
    "#             W_conv2 = self.get_variable(name=\"w\",scope=\"conv2\")\n",
    "#             b_conv2 = self.get_variable(name=\"b\",scope=\"conv2\")\n",
    "\n",
    "#             h_conv2 = tf.nn.relu(self.conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "#             h_pool2 = self.max_pool_2x2(h_conv2)\n",
    "#             self.h_pool2_flat = tf.reshape(h_pool2, [-1, 15*15*64])\n",
    "\n",
    "            # fc layer expansion\n",
    "            for layer in range(1, self.den_layers+1):\n",
    "\n",
    "                if layer == 1:\n",
    "                    w_fc1 = self.get_variable(name=\"w\", scope=\"l%d\"%layer)\n",
    "                    b_fc1 = self.get_variable(name=\"b\", scope=\"l%d\"%layer)\n",
    "\n",
    "\n",
    "                    w_expand = self.create_variable(name=\"w_ex_\"+str(task_id),shape=[w_fc1.get_shape().as_list()[0], self.k_ex], scope=\"l\"+str(layer))\n",
    "                    b_expand = self.create_variable(name=\"b_ex_\"+str(task_id),shape=[self.k_ex], scope=\"l%d\"%layer)\n",
    "                    w_expanded = tf.concat([w_fc1,w_expand],1)\n",
    "                    b_expanded = tf.concat([b_fc1,b_expand],0)\n",
    "                    self.params[w_fc1.name] = w_expanded\n",
    "                    self.params[b_fc1.name] = b_expanded\n",
    "                    h_fc1 = tf.nn.relu(tf.matmul(self.x, w_expanded) + b_expanded)\n",
    "\n",
    "                elif layer == self.den_layers:\n",
    "                    w_fc3 = self.get_variable(name=\"w_%d\"%task_id,scope=\"l%d\"%layer)\n",
    "                    b_fc3 = self.get_variable(name=\"b_%d\"%task_id,scope=\"l%d\"%layer)\n",
    "\n",
    "                    \n",
    "                    prev_dim = w_fc3.get_shape().as_list()[0]\n",
    "                    next_dim = w_fc3.get_shape().as_list()[1]\n",
    "\n",
    "                    w_expand = self.create_variable(name=\"w_ex_\"+str(task_id), shape=[self.k_ex, next_dim], scope=\"l%d\"%layer)\n",
    "                    \n",
    "                    w_expanded = tf.concat([w_fc3, w_expand], 0)\n",
    "                    \n",
    "                    self.params[w_fc3.name] = w_expanded\n",
    "                    y_conv = tf.matmul(self.h_fc2, w_expanded) + b_fc3\n",
    "                    \n",
    "                    # Dummy nodes for previous outputs\n",
    "                    w_fc3_old = self.get_variable(name=\"w\",scope=\"l%d\"%layer)\n",
    "                    prev_old = w_fc3_old.get_shape().as_list()[0]\n",
    "                    next_old = w_fc3_old.get_shape().as_list()[1]\n",
    "                    dummy_w_old = tf.get_variable(name=\"dummy_t%d_l%d\" %(task_id,layer), shape=[self.k_ex, next_old], \n",
    "                                initializer=tf.constant_initializer(0.0), trainable=False)\n",
    "                    w_fc3_dummy = tf.concat([w_fc3_old, dummy_w_old],0)\n",
    "                    self.params[w_fc3_old.name] = w_fc3_dummy\n",
    "\n",
    "                else:\n",
    "                    w_fc2 = self.get_variable(name=\"w\",scope=\"l%d\"%layer)\n",
    "                    b_fc2 = self.get_variable(name=\"b\",scope=\"l%d\"%layer)\n",
    "\n",
    "                    prev_dim = w_fc2.get_shape().as_list()[0]\n",
    "                    next_dim = w_fc2.get_shape().as_list()[1]\n",
    "                    \n",
    "                    # Dummy nodes for prev hidden nodes\n",
    "                    dummy_w = tf.get_variable(name=\"dummy_t%d_l%d\" %(task_id,layer), shape=[self.k_ex, next_dim], \n",
    "                                initializer=tf.constant_initializer(0.0), trainable=False)\n",
    "\n",
    "                    w_expand = self.create_variable(name=\"w_ex_\"+str(task_id), shape=[prev_dim + self.k_ex, self.k_ex], scope=\"l%d\"%layer)\n",
    "                    b_expand = self.create_variable(name=\"b_ex_\"+str(task_id), shape=[self.k_ex], scope=\"l%d\"%layer)\n",
    "                    \n",
    "                    w_fc2_dummy = tf.concat([w_fc2, dummy_w],0)\n",
    "                    \n",
    "                    w_expanded = tf.concat([w_fc2_dummy, w_expand], 1)\n",
    "                    b_expanded = tf.concat([b_fc2, b_expand], 0)\n",
    "                    \n",
    "                    self.params[w_fc2.name] = w_expanded\n",
    "                    self.params[b_fc2.name] = b_expanded\n",
    "                    self.h_fc2 = tf.nn.relu(tf.matmul(h_fc1, w_expanded) + b_expanded)\n",
    "                    \n",
    "        else:\n",
    "\n",
    "#             #First Convolutional layers\n",
    "#             W_conv1 = self.get_variable(name=\"w\",scope=\"conv1\")\n",
    "#             b_conv1 = self.get_variable(name=\"b\",scope=\"conv1\")\n",
    "\n",
    "#             h_conv1 = tf.nn.relu(self.conv2d(self.x, W_conv1) + b_conv1)\n",
    "#             h_pool1 = self.max_pool_2x2(h_conv1)\n",
    "\n",
    "#             #Second Convolutional Layer\n",
    "#             W_conv2 = self.get_variable(name=\"w\",scope=\"conv2\")\n",
    "#             b_conv2 = self.get_variable(name=\"b\",scope=\"conv2\")\n",
    "\n",
    "#             h_conv2 = tf.nn.relu(self.conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "#             h_pool2 = self.max_pool_2x2(h_conv2)\n",
    "#             self.h_pool2_flat = tf.reshape(h_pool2, [-1, 15*15*64])\n",
    "\n",
    "            #flattened first fc layer\n",
    "            W_fc1 = self.get_variable(name=\"w\", scope=\"l1\")   # layer-1 outgoing weight matrix\n",
    "            b_fc1 = self.get_variable(name=\"b\", scope=\"l1\")\n",
    "\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(self.x, W_fc1) + b_fc1)\n",
    "\n",
    "            #second fc layer\n",
    "            W_fc2 = self.get_variable(name=\"w\", scope=\"l2\")   # layer-2 outgoing weight matrix\n",
    "            b_fc2 = self.get_variable(name=\"b\", scope=\"l2\")\n",
    "\n",
    "            self.h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "\n",
    "            self.w_fc = self.create_variable(name=\"w_\"+str(task_id), shape=[128, output_len], scope=\"l3\", trainable=True)     # layer-3 outgoing weight matrix \n",
    "            self.b_fc = self.create_variable(name=\"b_\"+str(task_id), shape=[output_len], scope=\"l3\", trainable=True)\n",
    "            y_conv = tf.matmul(self.h_fc2, self.w_fc) + self.b_fc\n",
    "\n",
    "#         y_conv = tf.nn.sigmoid(y_conv)\n",
    "        return y_conv\n",
    "    \n",
    "    def perform_selection(self, task_id, values_dict):      # Breadth first search for selecting non-zero units\n",
    "        \n",
    "        all_indices = defaultdict(list)     # to store indices of nonzero units\n",
    "        selected_params = dict()            # to store values of nonzero units\n",
    "        selected_prev_params = dict()\n",
    "        \n",
    "#         for scope, value in values_dict.items():    # Storing conv layers in selected parameters \n",
    "#             if \"conv\" in scope:\n",
    "#                 selected_params[scope] = value\n",
    "            \n",
    "        for i in reversed(range(1,self.den_layers+1)):\n",
    "            if i == self.den_layers:\n",
    "                w = values_dict['l%d/w_%d:0' %(i,task_id)]\n",
    "                b = values_dict['l%d/b_%d:0' %(i,task_id)]\n",
    "                for j in range(w.shape[0]):\n",
    "                    if w[j,0] != 0:\n",
    "                        all_indices['l%d' % i].append(j)\n",
    "                # np.ix_(): fancy indexing, index with arrays of integers\n",
    "                # Select non-zero weights between the last hidden layer and the output layer\n",
    "                selected_params['l%d/w_%d:0' % (i, task_id)] = \\\n",
    "                    w[np.ix_(all_indices['l%d' % i], [0])]\n",
    "                selected_params['l%d/b_%d:0' % (i, task_id)] = b\n",
    "            else:\n",
    "                w = values_dict['l%d/w:0' % i]\n",
    "                b = values_dict['l%d/b:0' % i]\n",
    "                top_indices = all_indices['l%d' % (i + 1)]\n",
    "                print(len(top_indices))\n",
    "                for j in range(w.shape[0]):\n",
    "                    if np.count_nonzero(w[j, top_indices]) != 0 or i == 1:\n",
    "                        all_indices['l%d' % i].append(j)\n",
    "                \n",
    "                # non-zero weights between the layer i and the layer i+1\n",
    "                sub_weight = w[np.ix_(all_indices['l%d' % i], top_indices)]\n",
    "                sub_biases = b[all_indices['l%d' % (i + 1)]]\n",
    "                selected_params['l%d/w:0' % i] = sub_weight\n",
    "                selected_params['l%d/b:0' % i] = sub_biases\n",
    "                \n",
    "                # prev_W : to avoid drastic change in value of weights (Regularization)\n",
    "                selected_prev_params['l%d/w:0' % i] = \\\n",
    "                    self.prev_W['l%d/w:0' % i][np.ix_(all_indices['l%d' % i], top_indices)]\n",
    "                selected_prev_params['l%d/b:0' % i] = \\\n",
    "                    self.prev_W['l%d/b:0' % i][all_indices['l%d' % (i + 1)]]\n",
    "\n",
    "#         for keys, value in selected_params.items():\n",
    "#             print(keys)\n",
    "#             print(value)\n",
    "                \n",
    "        return [selected_params, selected_prev_params, all_indices]\n",
    "        \n",
    "    def build_SR(self, task_id, selected, output_len):    # creating selective retraining model\n",
    "        \n",
    "        self.initialize_parameters(output_len)\n",
    "        h = self.x\n",
    "        \n",
    "        # conv layers\n",
    "#         for i in range(1, self.conv_layers+1):\n",
    "#             with tf.variable_scope('conv%d' % i):\n",
    "#                 w = tf.get_variable('w', initializer=selected['conv%d/w:0' % i], trainable=False)\n",
    "#                 b = tf.get_variable('b', initializer=selected['conv%d/b:0' % i], trainable=False)\n",
    "#             h_conv = tf.nn.relu(self.conv2d(h, w) + b)\n",
    "#             h = self.max_pool_2x2(h_conv)\n",
    "            \n",
    "#         # flattening\n",
    "#         h = tf.reshape(h, [-1, 15*15*64])\n",
    "        \n",
    "        for i in range(1, self.den_layers):\n",
    "            with tf.variable_scope('l%d' % i):\n",
    "                w = tf.get_variable('w', initializer=selected['l%d/w:0' % i], trainable=True)\n",
    "                b = tf.get_variable('b', initializer=selected['l%d/b:0' % i], trainable=True)\n",
    "            h = tf.nn.relu(tf.matmul(h, w) + b)\n",
    "            \n",
    "        # last layer\n",
    "        with tf.variable_scope('l%d' % self.den_layers):\n",
    "            w = tf.get_variable('w_%d' % task_id,\n",
    "                                initializer=selected['l%d/w_%d:0' % (self.den_layers, task_id)], trainable=True)\n",
    "            b = tf.get_variable('b_%d' % task_id,\n",
    "                                initializer=selected['l%d/b_%d:0' % (self.den_layers, task_id)], trainable=True)\n",
    "\n",
    "        y_conv = tf.matmul(h, w) + b\n",
    "#         y_conv = tf.nn.sigmoid(y_conv)\n",
    "        return y_conv\n",
    "\n",
    "    def optimization(self, prev_W=None):\n",
    "\n",
    "        l2_regular = 0\n",
    "        train_var = []\n",
    "        regular_terms = []\n",
    "        \n",
    "        self.loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(labels=self.y_, logits=y_conv))\n",
    "\n",
    "        for var in tf.trainable_variables():\n",
    "            l2_regular = l2_regular + tf.nn.l2_loss(var)\n",
    "            train_var.append(var)\n",
    "#         print(len(train_var))\n",
    "\n",
    "        self.loss = self.loss + tf.reduce_mean(self.l2_mu * l2_regular)\n",
    "\n",
    "        if prev_W != None:\n",
    "            for var in train_var:\n",
    "                if var.name in prev_W.keys():\n",
    "                    prev_w = prev_W[var.name]\n",
    "                    regular_terms.append(tf.nn.l2_loss(var - prev_w))\n",
    "            self.loss = self.loss + self.lamba_regular * tf.reduce_mean(regular_terms)\n",
    "        \n",
    "        opt = tf.train.AdamOptimizer(1e-5)\n",
    "        grads = opt.compute_gradients(self.loss, train_var)\n",
    "        apply_grads = opt.apply_gradients(grads)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(self.y_,1))\n",
    "        self.acc_train = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        l1_var = [var for var in tf.trainable_variables()]\n",
    "        l1_op_list = []\n",
    "        with tf.control_dependencies([apply_grads]):  # exec apply_grads first\n",
    "            for var in l1_var:\n",
    "                th_t = tf.fill(tf.shape(var), tf.convert_to_tensor(self.l1_thr))\n",
    "                zero_t = tf.zeros(tf.shape(var))\n",
    "                var_temp = var - (th_t * tf.sign(var))\n",
    "                # [pseudo]:  if |var| < th_t: var = [0];  else: var = var_temp\n",
    "                l1_op = var.assign(tf.where(tf.less(tf.abs(var), th_t), zero_t, var_temp))\n",
    "                l1_op_list.append(l1_op)\n",
    "        \n",
    "        with tf.control_dependencies(l1_op_list):\n",
    "            self.train_model = tf.no_op()\n",
    "\n",
    "    def train_task(self, task_id, batch_size, epochs):\n",
    "\n",
    "        if task_id == 1:\n",
    "            task_train_labels = tf.one_hot(indices=np.array(self.train_labels), depth=self.last_label_index+1)\n",
    "        else:\n",
    "            task_train_labels = np.array(self.train_labels)\n",
    "            task_train_labels = task_train_labels.reshape((task_train_labels.shape[0],1))\n",
    "            task_train_labels = tf.convert_to_tensor(task_train_labels)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((np.array(self.train), task_train_labels))\n",
    "        dataset = dataset.shuffle(len(self.train_labels)).repeat().batch(batch_size)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        (x_data , y_data) = iterator.get_next()\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        task_train_labels = self.sess.run(task_train_labels)\n",
    "        count = 0\n",
    "\n",
    "#         for scope, ref in self.params.items():\n",
    "#             if \"l2\" in scope:\n",
    "#                 print(self.sess.run(ref))\n",
    "\n",
    "        for i in range(epochs):\n",
    "\n",
    "            for j in range(int(len(self.train)/batch_size)):\n",
    "                x_batch , y_batch = self.sess.run([x_data,y_data])\n",
    "                _, loss = self.sess.run([self.train_model, self.loss], feed_dict={self.x: x_batch, self.y_: y_batch})\n",
    "\n",
    "            train_accuracy = self.acc_train.eval(session=self.sess, feed_dict={self.x: self.train, self.y_: task_train_labels})\n",
    "            print(\"Epoch %d, training accuracy %g\"%(i+1, train_accuracy))\n",
    "\n",
    "            if train_accuracy == 1:\n",
    "                count += 1\n",
    "\n",
    "                if count > 2:\n",
    "                    print(\"Best accuracy achieved! \\n\")\n",
    "                    break\n",
    "        \n",
    "#         for scope, ref in self.params.items():\n",
    "#             if \"l2\" in scope:\n",
    "#                 print(self.sess.run(ref))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def predict(self, task_id, output_len=2):\n",
    "\n",
    "        self.initialize_parameters(output_len)\n",
    "\n",
    "        task_test_labels = tf.one_hot(indices=self.test_labels, depth=self.last_label_index+1)\n",
    "        task_test_labels = self.sess.run(task_test_labels)\n",
    "        \n",
    "#         #First Convolutional layers\n",
    "#         W_conv1 = self.get_variable(name=\"w\",scope=\"conv1\")\n",
    "#         b_conv1 = self.get_variable(name=\"b\",scope=\"conv1\")\n",
    "\n",
    "#         h_conv1 = tf.nn.relu(self.conv2d(self.x, W_conv1) + b_conv1)\n",
    "#         h_pool1 = self.max_pool_2x2(h_conv1)\n",
    "\n",
    "#         #Second Convolutional Layer\n",
    "#         W_conv2 = self.get_variable(name=\"w\",scope=\"conv2\")\n",
    "#         b_conv2 = self.get_variable(name=\"b\",scope=\"conv2\")\n",
    "\n",
    "#         h_conv2 = tf.nn.relu(self.conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "#         h_pool2 = self.max_pool_2x2(h_conv2)\n",
    "#         self.h_pool2_flat = tf.reshape(h_pool2, [-1, 15*15*64])\n",
    "\n",
    "        #flattened first fc layer\n",
    "        W_fc1 = self.get_variable(name=\"w\", scope=\"l1\")   # layer-1 outgoing weight matrix\n",
    "        b_fc1 = self.get_variable(name=\"b\", scope=\"l1\")\n",
    "\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(self.x, W_fc1) + b_fc1)\n",
    "\n",
    "        #second fc layer\n",
    "        W_fc2 = self.get_variable(name=\"w\", scope=\"l2\")   # layer-2 outgoing weight matrix\n",
    "        b_fc2 = self.get_variable(name=\"b\", scope=\"l2\")\n",
    "\n",
    "        self.h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "\n",
    "        self.w_fc = self.get_variable(name=\"w\", scope=\"l3\")\n",
    "        self.b_fc = self.get_variable(name=\"b\", scope=\"l3\")\n",
    "\n",
    "        y_final = tf.matmul(self.h_fc2, self.w_fc) + self.b_fc\n",
    "        y_final = tf.nn.sigmoid(y_final)\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(y_final,1), tf.argmax(self.y_,1))\n",
    "        self.acc_test = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        test_accuracy = self.acc_test.eval(session=self.sess, feed_dict={self.x: self.test, self.y_: task_test_labels})\n",
    "        print(\"Overall accuracy: %g \\n\"%test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "..........loading dataset from numpy files..........\n",
      "\n",
      " \n",
      " Added new category: Apple Braeburn\n",
      " \n",
      " Added new category: Apple Golden 2\n",
      "-------------Training new task: 1--------------\n",
      "Epoch 1, training accuracy 1\n",
      "Epoch 2, training accuracy 1\n",
      "Epoch 3, training accuracy 1\n",
      "Best accuracy achieved! \n",
      "\n",
      "Overall accuracy: 1 \n",
      "\n",
      "\n",
      " y_final:\n",
      "[[0.925228   0.07350022]\n",
      " [0.806224   0.25826502]\n",
      " [0.91871595 0.09214067]\n",
      " [0.7900766  0.26587877]\n",
      " [0.8824437  0.14135212]\n",
      " [0.92689157 0.06902856]\n",
      " [0.7877852  0.27055275]\n",
      " [0.9388169  0.08070984]\n",
      " [0.8161255  0.23698762]\n",
      " [0.92573565 0.08440652]\n",
      " [0.9283543  0.07813153]\n",
      " [0.9219725  0.09528816]\n",
      " [0.93341076 0.07293212]\n",
      " [0.87660944 0.14726213]\n",
      " [0.8255601  0.19259912]\n",
      " [0.8757572  0.1456641 ]\n",
      " [0.8062161  0.26432645]\n",
      " [0.8173367  0.24697328]\n",
      " [0.93393075 0.0766471 ]\n",
      " [0.92728364 0.08010516]\n",
      " [0.9204519  0.08004925]\n",
      " [0.8648187  0.13353223]\n",
      " [0.92786086 0.07931688]\n",
      " [0.93685955 0.08062032]\n",
      " [0.8491336  0.15994379]\n",
      " [0.92998904 0.08358434]\n",
      " [0.9378567  0.0788621 ]\n",
      " [0.8551223  0.1655179 ]\n",
      " [0.92420614 0.08469158]\n",
      " [0.88042617 0.14531362]\n",
      " [0.92564607 0.08250543]\n",
      " [0.93172336 0.068802  ]\n",
      " [0.82024455 0.23520246]\n",
      " [0.92203957 0.09167793]\n",
      " [0.7744237  0.2707227 ]\n",
      " [0.86010706 0.14476565]\n",
      " [0.92945486 0.07838061]\n",
      " [0.8314402  0.22040483]\n",
      " [0.8740362  0.13849753]\n",
      " [0.81812143 0.25067192]\n",
      " [0.93167347 0.07578388]\n",
      " [0.79505026 0.2707892 ]\n",
      " [0.92715096 0.08584803]\n",
      " [0.8634286  0.13611037]\n",
      " [0.91926694 0.09192225]\n",
      " [0.93026495 0.06897104]\n",
      " [0.9147365  0.08303547]\n",
      " [0.92973346 0.07194489]\n",
      " [0.92814624 0.09106174]\n",
      " [0.78500795 0.2852953 ]\n",
      " [0.86226016 0.14741847]\n",
      " [0.9255241  0.07375649]\n",
      " [0.85731673 0.1366356 ]\n",
      " [0.87300503 0.13659093]\n",
      " [0.7939218  0.22742876]\n",
      " [0.9273175  0.0694201 ]\n",
      " [0.88069904 0.14422312]\n",
      " [0.84916925 0.15469453]\n",
      " [0.81153065 0.25080687]\n",
      " [0.92524683 0.08961985]\n",
      " [0.9197329  0.08802393]\n",
      " [0.8685693  0.13390297]\n",
      " [0.92887723 0.07611093]\n",
      " [0.77126396 0.292947  ]\n",
      " [0.92035043 0.08242473]\n",
      " [0.80424225 0.25259846]\n",
      " [0.8632133  0.13685453]\n",
      " [0.81066394 0.2562555 ]\n",
      " [0.92662054 0.08639881]\n",
      " [0.8269496  0.19887775]\n",
      " [0.79901075 0.2210418 ]\n",
      " [0.92737603 0.06905809]\n",
      " [0.7836394  0.2809024 ]\n",
      " [0.9299146  0.07130626]\n",
      " [0.92634887 0.08525831]\n",
      " [0.84907395 0.14967403]\n",
      " [0.8187362  0.21154213]\n",
      " [0.8201414  0.24288681]\n",
      " [0.92538816 0.08076698]\n",
      " [0.9295615  0.07755753]\n",
      " [0.76578367 0.28832114]\n",
      " [0.93636626 0.07988584]\n",
      " [0.86054957 0.14365393]\n",
      " [0.7988076  0.2552234 ]\n",
      " [0.9272121  0.08959475]\n",
      " [0.88380086 0.13197935]\n",
      " [0.9257457  0.07887793]\n",
      " [0.7980782  0.2707138 ]\n",
      " [0.934789   0.06587708]\n",
      " [0.9286926  0.07115641]\n",
      " [0.91698515 0.09401828]\n",
      " [0.91383827 0.09951144]\n",
      " [0.92521775 0.08104518]\n",
      " [0.92324865 0.0834403 ]\n",
      " [0.92643005 0.08874837]\n",
      " [0.8639549  0.1485216 ]\n",
      " [0.92209554 0.08833537]\n",
      " [0.9314351  0.0747456 ]\n",
      " [0.84630525 0.16453373]\n",
      " [0.9226452  0.08066598]\n",
      " [0.92993534 0.07987726]\n",
      " [0.78309155 0.28958076]\n",
      " [0.92589724 0.08192819]\n",
      " [0.79009104 0.27717495]\n",
      " [0.93018794 0.07727915]\n",
      " [0.8597435  0.14379847]\n",
      " [0.79627496 0.27823198]\n",
      " [0.8617135  0.1416477 ]\n",
      " [0.86638427 0.14495784]\n",
      " [0.9206838  0.09019375]\n",
      " [0.8845916  0.12613195]\n",
      " [0.8650588  0.15335777]\n",
      " [0.9070604  0.1004234 ]\n",
      " [0.9120767  0.08678117]\n",
      " [0.87519956 0.14942563]\n",
      " [0.82554966 0.24094677]\n",
      " [0.91963524 0.09052715]\n",
      " [0.92070454 0.08694425]\n",
      " [0.8015182  0.26342285]\n",
      " [0.8591491  0.1579229 ]\n",
      " [0.8591575  0.16210863]\n",
      " [0.87601244 0.13981232]\n",
      " [0.8226358  0.22684833]\n",
      " [0.8151189  0.2438032 ]\n",
      " [0.91967595 0.09206352]\n",
      " [0.77461755 0.27258113]\n",
      " [0.9299811  0.07601944]\n",
      " [0.8239875  0.23036048]\n",
      " [0.8563504  0.14301383]\n",
      " [0.9239519  0.08904746]\n",
      " [0.9227359  0.07923666]\n",
      " [0.81226784 0.25165576]\n",
      " [0.9333779  0.07355696]\n",
      " [0.9196514  0.08868375]\n",
      " [0.93047285 0.08196923]\n",
      " [0.8665559  0.1419746 ]\n",
      " [0.9167783  0.08086377]\n",
      " [0.81055415 0.21601576]\n",
      " [0.92728806 0.08165315]\n",
      " [0.925315   0.06902525]\n",
      " [0.7779353  0.24713996]\n",
      " [0.91454875 0.09400186]\n",
      " [0.8750552  0.1525729 ]\n",
      " [0.86289597 0.14974219]\n",
      " [0.79169124 0.2661289 ]\n",
      " [0.84760344 0.17372611]\n",
      " [0.8260058  0.19932064]\n",
      " [0.9281776  0.06968626]\n",
      " [0.86101735 0.15924075]\n",
      " [0.9251779  0.07030478]\n",
      " [0.92743754 0.08019611]\n",
      " [0.9221796  0.0724245 ]\n",
      " [0.9210166  0.08308393]\n",
      " [0.919312   0.08275646]\n",
      " [0.8476067  0.15539971]\n",
      " [0.9239507  0.08344784]\n",
      " [0.8648238  0.15250975]\n",
      " [0.92902046 0.08766967]\n",
      " [0.76809704 0.29296517]\n",
      " [0.9257238  0.07888228]\n",
      " [0.9301069  0.08228841]\n",
      " [0.852556   0.15393794]\n",
      " [0.92572176 0.06939259]\n",
      " [0.7820835  0.25603434]\n",
      " [0.07879519 0.91646516]\n",
      " [0.08899379 0.9169921 ]\n",
      " [0.06872344 0.90371627]\n",
      " [0.12240028 0.8810792 ]\n",
      " [0.08683282 0.9023031 ]\n",
      " [0.07460189 0.9181392 ]\n",
      " [0.1361404  0.8730029 ]\n",
      " [0.06350392 0.919391  ]\n",
      " [0.09172881 0.92012405]\n",
      " [0.06321603 0.9189635 ]\n",
      " [0.06273055 0.93020916]\n",
      " [0.07297161 0.90278745]\n",
      " [0.06388578 0.92450666]\n",
      " [0.0926401  0.89711004]\n",
      " [0.09650025 0.8732096 ]\n",
      " [0.09420294 0.89483553]\n",
      " [0.09630835 0.90624404]\n",
      " [0.08785835 0.91826236]\n",
      " [0.05705702 0.9340496 ]\n",
      " [0.05770648 0.93498063]\n",
      " [0.07655808 0.9183508 ]\n",
      " [0.11097237 0.8787987 ]\n",
      " [0.05796713 0.93562156]\n",
      " [0.06802776 0.91559863]\n",
      " [0.09289458 0.90440917]\n",
      " [0.0688934  0.91199595]\n",
      " [0.06708673 0.91320634]\n",
      " [0.09680259 0.88418674]\n",
      " [0.06813774 0.9141768 ]\n",
      " [0.0904538  0.8969797 ]\n",
      " [0.069801   0.9128235 ]\n",
      " [0.07172546 0.92189837]\n",
      " [0.10929331 0.9123226 ]\n",
      " [0.06827459 0.9133649 ]\n",
      " [0.09783664 0.90322816]\n",
      " [0.08951005 0.9019413 ]\n",
      " [0.06551024 0.92191505]\n",
      " [0.07172036 0.9311403 ]\n",
      " [0.08974239 0.89917934]\n",
      " [0.09558141 0.90880656]\n",
      " [0.06087315 0.93137866]\n",
      " [0.08779064 0.9209198 ]\n",
      " [0.07042179 0.9124479 ]\n",
      " [0.10943827 0.8635807 ]\n",
      " [0.07151556 0.9140092 ]\n",
      " [0.06915781 0.9185989 ]\n",
      " [0.06182286 0.925882  ]\n",
      " [0.07057938 0.9187369 ]\n",
      " [0.07022986 0.9078207 ]\n",
      " [0.1196723  0.8877094 ]\n",
      " [0.08691236 0.9068515 ]\n",
      " [0.05945858 0.92634857]\n",
      " [0.10812891 0.86525744]\n",
      " [0.10309273 0.88919795]\n",
      " [0.09536594 0.89745104]\n",
      " [0.0729802  0.91513336]\n",
      " [0.09450036 0.8902181 ]\n",
      " [0.11283988 0.86300933]\n",
      " [0.08241034 0.9248605 ]\n",
      " [0.06814355 0.91462606]\n",
      " [0.06949255 0.9082097 ]\n",
      " [0.11174473 0.8811338 ]\n",
      " [0.07095334 0.92552894]\n",
      " [0.13388479 0.8753718 ]\n",
      " [0.06451663 0.9269595 ]\n",
      " [0.09511536 0.8998006 ]\n",
      " [0.11541706 0.8705404 ]\n",
      " [0.08925685 0.9180445 ]\n",
      " [0.06854305 0.9106939 ]\n",
      " [0.09963617 0.8726531 ]\n",
      " [0.09749547 0.8944005 ]\n",
      " [0.06322232 0.9261986 ]\n",
      " [0.1397376  0.86930776]\n",
      " [0.07566658 0.9176799 ]\n",
      " [0.06766957 0.90515614]\n",
      " [0.11161521 0.86269957]\n",
      " [0.09738261 0.87228817]\n",
      " [0.11242896 0.912873  ]\n",
      " [0.0642314  0.92986465]\n",
      " [0.06484395 0.9298346 ]\n",
      " [0.13807854 0.8733425 ]\n",
      " [0.06775254 0.9140432 ]\n",
      " [0.11929905 0.85499346]\n",
      " [0.09697554 0.9001975 ]\n",
      " [0.07136953 0.9093543 ]\n",
      " [0.0922699  0.8968085 ]\n",
      " [0.06746915 0.91187   ]\n",
      " [0.08894753 0.912028  ]\n",
      " [0.05816615 0.92882866]\n",
      " [0.07729921 0.9176927 ]\n",
      " [0.07271513 0.9092134 ]\n",
      " [0.06223652 0.9201898 ]\n",
      " [0.06951144 0.9099413 ]\n",
      " [0.07378441 0.9052516 ]\n",
      " [0.06907067 0.90465623]\n",
      " [0.11000726 0.8613714 ]\n",
      " [0.06213754 0.9215504 ]\n",
      " [0.06576824 0.92994636]\n",
      " [0.09652025 0.8929541 ]\n",
      " [0.07318026 0.91909564]\n",
      " [0.06293753 0.91531503]\n",
      " [0.10497954 0.89782286]\n",
      " [0.06518608 0.91412437]\n",
      " [0.1357187  0.8751987 ]\n",
      " [0.06463715 0.9274822 ]\n",
      " [0.09487376 0.9002862 ]\n",
      " [0.11436436 0.8903022 ]\n",
      " [0.11027083 0.8797742 ]\n",
      " [0.10063598 0.8753272 ]\n",
      " [0.06560406 0.92189515]\n",
      " [0.11672619 0.86914206]\n",
      " [0.09434986 0.88499665]\n",
      " [0.07163358 0.9134262 ]\n",
      " [0.05996329 0.9254303 ]\n",
      " [0.08629471 0.9003446 ]\n",
      " [0.09131375 0.9159605 ]\n",
      " [0.06648153 0.92045265]\n",
      " [0.07252386 0.9147651 ]\n",
      " [0.09887204 0.9146411 ]\n",
      " [0.10149986 0.8817924 ]\n",
      " [0.10025913 0.87264544]\n",
      " [0.11109391 0.87888026]\n",
      " [0.08989176 0.9223231 ]\n",
      " [0.09726077 0.9168782 ]\n",
      " [0.07026148 0.90450215]\n",
      " [0.0959824  0.9019787 ]\n",
      " [0.06093544 0.93149745]\n",
      " [0.09659114 0.9167992 ]\n",
      " [0.11290869 0.87064326]\n",
      " [0.06218478 0.91735387]\n",
      " [0.06781149 0.91554886]\n",
      " [0.09876576 0.90959346]\n",
      " [0.07048717 0.92308354]\n",
      " [0.06889719 0.92144716]\n",
      " [0.0676364  0.91465235]\n",
      " [0.11658356 0.85103035]\n",
      " [0.06153157 0.9235216 ]\n",
      " [0.09784618 0.8686985 ]\n",
      " [0.06456777 0.91604   ]\n",
      " [0.06140211 0.9257671 ]\n",
      " [0.09243277 0.90676486]\n",
      " [0.06383887 0.9158242 ]\n",
      " [0.09313926 0.8917003 ]\n",
      " [0.08999324 0.9025103 ]\n",
      " [0.09864786 0.9023076 ]\n",
      " [0.10305205 0.86645484]\n",
      " [0.1020917  0.86713445]\n",
      " [0.07139242 0.9244482 ]\n",
      " [0.08956996 0.89601666]\n",
      " [0.05960658 0.92719954]\n",
      " [0.07025239 0.9127753 ]\n",
      " [0.06075665 0.9246008 ]\n",
      " [0.07356262 0.9167851 ]\n",
      " [0.06646395 0.9163822 ]\n",
      " [0.10777834 0.8642914 ]\n",
      " [0.062511   0.91875017]\n",
      " [0.10162857 0.8859112 ]\n",
      " [0.06977916 0.90924096]\n",
      " [0.12444398 0.87928736]\n",
      " [0.06033215 0.9327211 ]\n",
      " [0.06807241 0.91233635]\n",
      " [0.09057617 0.89908886]\n",
      " [0.0559454  0.9305668 ]\n",
      " [0.09814656 0.89702404]]\n",
      " \n",
      " Added new category: Mulberry\n",
      "-------------Training new task: 2--------------\n",
      "-----------Started Selective Retraining-------------\n",
      "---- Selecting nodes ----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, training accuracy 1\n",
      "Epoch 2, training accuracy 1\n",
      "Epoch 3, training accuracy 1\n",
      "Best accuracy achieved! \n",
      "\n",
      "126\n",
      "1024\n",
      "---- Retraining selected nodes ----\n",
      "Epoch 1, training accuracy 1\n",
      "Epoch 2, training accuracy 1\n",
      "Epoch 3, training accuracy 1\n",
      "Best accuracy achieved! \n",
      "\n",
      "Loss: 5.729002\n",
      "Overall accuracy: 1 \n",
      "\n",
      "\n",
      " y_final:\n",
      "[[0.93125176 0.13516593 0.03578264]\n",
      " [0.8756962  0.27121288 0.02937061]\n",
      " [0.93052745 0.15139315 0.02289772]\n",
      " ...\n",
      " [0.44702315 0.20715624 0.9842502 ]\n",
      " [0.45609093 0.25040233 0.98646927]\n",
      " [0.46711195 0.24774769 0.9792501 ]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # train_data_path = '../Fruit_dataset/Train'\n",
    "    # test_data_path = '../Fruit_dataset/Test'\n",
    "\n",
    "    # lesser categories for faster loading during testing the code\n",
    "    train_data_path = '../Fruit_dataset/temp_Train'\n",
    "    test_data_path = '../Fruit_dataset/temp_Test'\n",
    "\n",
    "    # For easier disk read operation\n",
    "    np_train_path = '../Fruit_dataset/numpy_dataset/train.npy.gz'\n",
    "    np_test_path = '../Fruit_dataset/numpy_dataset/test.npy.gz'\n",
    "    np_train_label_path = '../Fruit_dataset/numpy_dataset/train_labels.npy'\n",
    "    np_test_label_path = '../Fruit_dataset/numpy_dataset/test_labels.npy'\n",
    "    np_label_name_path = '../Fruit_dataset/numpy_dataset/label_names.npy'\n",
    "\n",
    "    batch_size = 10\n",
    "    epochs = 5\n",
    "    early_stop = 5\n",
    "\n",
    "    den = DEN()\n",
    "\n",
    "    if os.path.exists(np_train_path):\n",
    "\n",
    "        print(\"\\n..........loading dataset from numpy files..........\\n\")\n",
    "\n",
    "        with gzip.GzipFile(np_train_path, \"r\") as f:\n",
    "            train_data = np.load(f)\n",
    "        with gzip.GzipFile(np_test_path, \"r\") as f:\n",
    "            test_data = np.load(f)\n",
    "\n",
    "        train_labels = np.load(np_train_label_path)\n",
    "        test_labels = np.load(np_test_label_path)\n",
    "        label_names = np.load(np_label_name_path)\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(\"\\n..........loading dataset from disk..........\\n\")\n",
    "        train_data, train_labels, label_names = den.extract_data(train_data_path)\n",
    "        test_data, test_labels, _ = den.extract_data(test_data_path)\n",
    "\n",
    "        os.makedirs(os.path.dirname(np_train_path), exist_ok=True)\n",
    "\n",
    "        with gzip.GzipFile(np_train_path, \"w\") as f:\n",
    "            np.save(f, train_data)\n",
    "        with gzip.GzipFile(np_test_path, \"w\") as f:\n",
    "            np.save(f, test_data)\n",
    "\n",
    "        np.save(np_train_label_path, train_labels)\n",
    "        np.save(np_test_label_path, test_labels)\n",
    "        np.save(np_label_name_path, label_names)\n",
    "\n",
    "\n",
    "# show image using cv\n",
    "    # print(train_labels[512])\n",
    "    # cv.imshow(\"\", train_data[512])\n",
    "    # cv.waitKey(0)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    task_id = 0\n",
    "    y_conv = None\n",
    "\n",
    "    while den.last_label_index != (len(label_names) - 1):     # Loop for adding new tasks (lifelong learning)\n",
    "\n",
    "        task_id += 1\n",
    "        den.add_task(task_id, label_names)\n",
    "        param_values = dict()\n",
    "        selected = dict()\n",
    "        print(\"-------------Training new task: %d--------------\"%task_id)\n",
    "        if task_id == 1:\n",
    "            y_conv = den.build_model(task_id=task_id)\n",
    "            den.optimization()\n",
    "            _ = den.train_task(task_id=task_id, batch_size=batch_size, epochs=epochs)\n",
    "            params = den.get_params()\n",
    "        else:\n",
    "            print(\"-----------Started Selective Retraining-------------\")\n",
    "            \n",
    "            #------------------Selection-------------------\n",
    "            y_conv = den.build_model(task_id=task_id, output_len=1)\n",
    "            den.optimization()\n",
    "            \n",
    "            print(\"---- Selecting nodes ----\")\n",
    "            _ = den.train_task(task_id=task_id, batch_size=batch_size, epochs=early_stop)\n",
    "            params = den.get_params()\n",
    "            if True:\n",
    "                [selected, selected_prev, all_indices] = den.perform_selection(task_id=task_id, values_dict=params)\n",
    "                den.destroy_graph()\n",
    "\n",
    "                #------------------Retraining-------------------\n",
    "                print(\"---- Retraining selected nodes ----\")\n",
    "                y_conv = den.build_SR(task_id=task_id, selected=selected, output_len=1) \n",
    "                den.optimization(prev_W=selected_prev) \n",
    "                loss = den.train_task(task_id=task_id, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "                print(\"Loss: %f\"%loss)\n",
    "            \n",
    "#             if loss < den.loss_thr:\n",
    "\n",
    "            \n",
    "                #--------------Performing Union----------------\n",
    "                _vars = [(var.name, den.sess.run(var)) for var in tf.trainable_variables() if 'l' in var.name]\n",
    "\n",
    "                for item in _vars:\n",
    "                    key, values = item\n",
    "                    selected[key] = values\n",
    "\n",
    "                for i in reversed(range(1, den.den_layers+1)):\n",
    "                    if i == den.den_layers:\n",
    "                        temp_weight = params['l%d/w_%d:0' % (i, task_id)]\n",
    "                        temp_weight[np.ix_(all_indices['l%d' % i], [0])] = \\\n",
    "                            selected['l%d/w_%d:0' % (i, task_id)]\n",
    "                        params['l%d/w_%d:0' % (i, task_id)] = temp_weight\n",
    "                        params['l%d/b_%d:0' % (i, task_id)] = \\\n",
    "                            selected['l%d/b_%d:0' % (i, task_id)]\n",
    "                        # Updating output matrix structure\n",
    "                        params['l%d/w:0' % (i)] = np.concatenate([params['l%d/w:0' % (i)],params['l%d/w_%d:0' % (i,task_id)]], axis=1).tolist()\n",
    "                        params['l%d/b:0' % (i)] = np.concatenate([params['l%d/b:0' % (i)],params['l%d/b_%d:0' % (i,task_id)]], axis=0).tolist()\n",
    "                    else:\n",
    "                        temp_weight = params['l%d/w:0' % i]\n",
    "                        temp_biases = params['l%d/b:0' % i]\n",
    "                        temp_weight[np.ix_(all_indices['l%d' % i], all_indices['l%d' % (i + 1)])] = \\\n",
    "                            selected['l%d/w:0' % i]\n",
    "                        temp_biases[all_indices['l%d' % (i + 1)]] = \\\n",
    "                            selected['l%d/b:0' % i]\n",
    "                        params['l%d/w:0' % i] = temp_weight\n",
    "                        params['l%d/b:0' % i] = temp_biases\n",
    "\n",
    "            else:\n",
    "                \n",
    "                print(\"\\n-----------Started Dynamic Expansion------------\")\n",
    "                den.destroy_graph()\n",
    "                den.restore_params(task_id=task_id, trainable=False, param_values=params)\n",
    "                y_conv = den.build_model(task_id=task_id, expansion=True, output_len=1)\n",
    "                den.optimization()\n",
    "                _ = den.train_task(task_id=task_id, batch_size=batch_size, epochs=early_stop)\n",
    "                params = den.get_params()\n",
    "                params['l%d/w:0' % den.den_layers] = np.concatenate([params['l%d/w:0' % den.den_layers],params['l%d/w_%d:0' % (den.den_layers,task_id)]], axis=1).tolist()\n",
    "                params['l%d/b:0' % den.den_layers] = np.concatenate([params['l%d/b:0' % den.den_layers],params['l%d/b_%d:0' % (den.den_layers,task_id)]], axis=0).tolist()\n",
    "\n",
    "        den.destroy_graph()\n",
    "        den.restore_params(trainable=False, param_values=params)    # Freezes all learned weights\n",
    "                    \n",
    "        # ---------Performance-----------\n",
    "        den.predict(task_id=task_id, output_len=den.last_label_index+1)\n",
    "                    \n",
    "#         param_values = den.get_params()         # Backing-up model weights that were trained upto current task\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
