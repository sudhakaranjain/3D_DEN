{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#To increase cell width of ipynb\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEN():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.last_label_index = 0    # tracks the label index upto which the model has been trained\n",
    "        self.params = dict()\n",
    "        self.k_ex = 3\n",
    "        self.den_layers = 3\n",
    "        self.conv_layers = 2\n",
    "        tf.reset_default_graph()\n",
    "        self.sess = None\n",
    "        self.train = []\n",
    "        self.train_labels = []\n",
    "        self.test = []\n",
    "        self.test_labels = []\n",
    "        self.selected = dict()\n",
    "        self.l2_mu = 0.01\n",
    "        self.lamba_regular = 0.5\n",
    "        self.l1_thr = 0.00001\n",
    "        self.loss_thr = 0.01\n",
    "        self.IMG_SIZE = 100\n",
    "        self.IMG_SHAPE = (self.IMG_SIZE, self.IMG_SIZE, 3)\n",
    "\n",
    "    def extract_data(self, filepath):\n",
    "        data = []\n",
    "        data_labels = []\n",
    "\n",
    "        for label in os.listdir(filepath):\n",
    "            label_path = os.path.join(filepath, label)\n",
    "            count = 0\n",
    "            list = len(os.listdir(label_path))\n",
    "            for img in os.listdir(label_path):\n",
    "                image_path = os.path.join(label_path, img)\n",
    "                image = cv.imread(image_path)\n",
    "                re_image = cv.resize(image, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                # grayImage = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "                # (thresh, BW) = cv.threshold(grayImage, 127, 255, cv.THRESH_BINARY)\n",
    "                data.append(re_image)\n",
    "                data_labels.append(label)\n",
    "\n",
    "        data = np.array(data, dtype=\"float\") / 255.0\n",
    "        features = self.vgg.predict(data)\n",
    "        features_flatten = features.reshape((features.shape[0], 3 * 3 * 512))\n",
    "        # data = data.reshape(data.shape[0], 50, 50, 3)\n",
    "        data_labels = np.array(data_labels)\n",
    "        label_names = np.unique(data_labels)\n",
    "        # data_labels = tf.one_hot(indices=data_labels, depth=10)\n",
    "        return features_flatten, data_labels, label_names\n",
    "\n",
    "\n",
    "    def add_task(self, task_id, label_names, initial_output=2):\n",
    "\n",
    "        new_label_indices = []\n",
    "        self.train = []\n",
    "        self.train_labels = []\n",
    "        self.train_ex_labels = []      \n",
    " \n",
    "        if task_id == 1:\n",
    "            self.new_train = []\n",
    "            self.new_train_labels = []\n",
    "            self.new_train_ex_labels = []\n",
    "            self.total = []\n",
    "            self.total_ex_labels = []\n",
    "            self.test = []\n",
    "            self.test_labels = []\n",
    "            for i in range(initial_output):   # By default, first task is a binary classification\n",
    "                new_label_indices.append(i)\n",
    "            self.last_label_index = i\n",
    "\n",
    "        else:\n",
    "            new_label_indices.append(self.last_label_index+1)\n",
    "            self.last_label_index = self.last_label_index + 1\n",
    "            \n",
    "            #saving the old training data\n",
    "            self.total = self.total + self.new_train\n",
    "            self.total_ex_labels = self.total_ex_labels + self.new_train_ex_labels\n",
    "            self.new_train = []\n",
    "            self.new_train_labels = []\n",
    "            self.new_train_ex_labels = []\n",
    "\n",
    "        for index in new_label_indices:\n",
    "            print(\" \\n Added new category: \"+str(label_names[index]))\n",
    "            l = 1 if task_id != 1 else index\n",
    "            for data, label in zip(train_data, train_labels):\n",
    "                if label_names[index] == label:\n",
    "                    self.new_train.append(data)\n",
    "                    self.new_train_labels.append(l)\n",
    "                    self.new_train_ex_labels.append(index)\n",
    "\n",
    "            for data, label in zip(test_data, test_labels):\n",
    "                if label_names[index] == label:\n",
    "                    self.test.append(data)\n",
    "                    self.test_labels.append(index)\n",
    "                    \n",
    "        # ------------------ Random sampling old training data ------------------\n",
    "        if task_id != 1:\n",
    "            print(len(self.total))\n",
    "            print(len(self.total_ex_labels))\n",
    "            sampled_indices = random.sample(range(len(self.total)), len(self.new_train))\n",
    "            for k in sampled_indices:\n",
    "                self.train.append(self.total[k])\n",
    "                self.train_labels.append(0)\n",
    "                self.train_ex_labels.append(self.total_ex_labels[k])\n",
    "                \n",
    "            self.train = self.train + self.new_train\n",
    "            self.train_labels = self.train_labels + self.new_train_labels\n",
    "            self.train_ex_labels = self.train_ex_labels + self.new_train_ex_labels\n",
    "        else:\n",
    "            self.train = self.new_train\n",
    "            self.train_labels = self.new_train_labels\n",
    "            self.train_ex_labels = self.new_train_ex_labels\n",
    "                \n",
    "\n",
    "    def destroy_graph(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.params = dict()\n",
    "\n",
    "    def initialize_parameters(self, output_len=2):\n",
    "        self.sess = tf.Session()\n",
    "        self.x = tf.placeholder(tf.float32, [None, 3 * 3 * 512])\n",
    "        self.y_ = tf.placeholder(tf.float32, [None, output_len])\n",
    "        # self.keep_prob = tf.placeholder(tf.float32)          # dropout probability\n",
    "\n",
    "    def create_variable(self, name=None, shape=None, scope=None, trainable=True):\n",
    "        with tf.variable_scope(scope, reuse=False):\n",
    "            w = tf.get_variable(name, shape=shape, \n",
    "#                                 initializer=tf.random_normal_initializer(mean=0, stddev=1, seed=3),\n",
    "                                trainable=trainable)\n",
    "            if \"ex\" not in name:\n",
    "                self.params[w.name] = w\n",
    "        return w\n",
    "\n",
    "    def get_variable(self, name=None, scope=None):\n",
    "        with tf.variable_scope(scope, reuse=True):\n",
    "            w = tf.get_variable(name)\n",
    "            if \"ex\" not in name:\n",
    "                self.params[w.name] = w\n",
    "        return w\n",
    "\n",
    "    def restore_params(self, task_id=None, trainable=True, param_values=dict()):\n",
    "        self.params = dict()\n",
    "        self.prev_W = dict()\n",
    "        for scope_name, value in param_values.items():\n",
    "            self.prev_W[scope_name] = value\n",
    "            scope_name = scope_name.split(':')[0]\n",
    "            [scope, name] = scope_name.split('/')\n",
    "\n",
    "            if task_id != None:\n",
    "                if ('l%d/w_%d' % (self.den_layers,task_id) in scope_name) or ('l%d/b_%d' % (self.den_layers,task_id) in scope_name):\n",
    "                    trainable = True\n",
    "                else:\n",
    "                    trainable = False\n",
    "\n",
    "            with tf.variable_scope(scope, reuse=False):\n",
    "                w = tf.get_variable(name, initializer=value, trainable=trainable)\n",
    "            self.params[w.name] = w\n",
    "\n",
    "    def get_params(self):\n",
    "        vdict = dict()\n",
    "        for scope_name, ref_w in self.params.items():\n",
    "            vdict[scope_name] = self.sess.run(ref_w)\n",
    "        return vdict\n",
    "\n",
    "    def conv2d(self, x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    def max_pool_2x2(self, x):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "    def build_model(self, task_id, expansion=False, output_len=2):\n",
    "\n",
    "        # Note: scope and name values are only given to DEN layers, not for fixed sized layers.\n",
    "\n",
    "        self.initialize_parameters(output_len)\n",
    "\n",
    "        if task_id == 1:\n",
    "\n",
    "            #flattened first fc layer\n",
    "            W_fc1 = self.create_variable(name=\"w\", shape=[3 * 3 * 512, 1024], scope=\"l1\")   # layer-1 outgoing weight matrix\n",
    "            b_fc1 = self.create_variable(name=\"b\", shape=[1024], scope=\"l1\")\n",
    "\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(self.x, W_fc1) + b_fc1)\n",
    "\n",
    "            #second fc layer\n",
    "            W_fc2 = self.create_variable(name=\"w\", shape=[1024, 128], scope=\"l2\")   # layer-2 outgoing weight matrix\n",
    "            b_fc2 = self.create_variable(name=\"b\", shape=[128], scope=\"l2\")\n",
    "\n",
    "            self.h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "\n",
    "            # readout fc layer\n",
    "            self.w_fc = self.create_variable(name=\"w\", shape=[128, output_len], scope=\"l3\")     # layer-3 outgoing weight matrix \n",
    "            self.b_fc = self.create_variable(name=\"b\", shape=[output_len], scope=\"l3\")\n",
    "            y_conv = tf.matmul(self.h_fc2, self.w_fc) + self.b_fc\n",
    "            \n",
    "        elif expansion:\n",
    "            \n",
    "            # fc layer expansion\n",
    "            for layer in range(1, self.den_layers+1):\n",
    "\n",
    "                if layer == 1:\n",
    "                    w_fc1 = self.get_variable(name=\"w\", scope=\"l%d\"%layer)\n",
    "                    b_fc1 = self.get_variable(name=\"b\", scope=\"l%d\"%layer)\n",
    "\n",
    "\n",
    "                    w_expand = self.create_variable(name=\"w_ex_\"+str(task_id),shape=[w_fc1.get_shape().as_list()[0], self.k_ex], scope=\"l\"+str(layer))\n",
    "                    b_expand = self.create_variable(name=\"b_ex_\"+str(task_id),shape=[self.k_ex], scope=\"l%d\"%layer)\n",
    "                    w_expanded = tf.concat([w_fc1,w_expand],1)\n",
    "                    b_expanded = tf.concat([b_fc1,b_expand],0)\n",
    "                    self.params[w_fc1.name] = w_expanded\n",
    "                    self.params[b_fc1.name] = b_expanded\n",
    "                    h_fc1 = tf.nn.relu(tf.matmul(self.x, w_expanded) + b_expanded)\n",
    "\n",
    "                elif layer == self.den_layers:\n",
    "                    #weight matrix of current task output\n",
    "                    w_fc3 = self.get_variable(name=\"w_%d\"%task_id,scope=\"l%d\"%layer)\n",
    "                    b_fc3 = self.get_variable(name=\"b_%d\"%task_id,scope=\"l%d\"%layer)\n",
    "                    prev_dim = w_fc3.get_shape().as_list()[0]\n",
    "                    next_dim = w_fc3.get_shape().as_list()[1]\n",
    "                    \n",
    "                    #weight matrix of old tasks output\n",
    "                    w_fc3_old = self.get_variable(name=\"w\",scope=\"l%d\"%layer)\n",
    "                    b_fc3_old = self.get_variable(name=\"b\",scope=\"l%d\"%layer)\n",
    "                    prev_old = w_fc3_old.get_shape().as_list()[0]\n",
    "                    next_old = w_fc3_old.get_shape().as_list()[1]\n",
    "                    \n",
    "                    w_merge = tf.concat([w_fc3_old, w_fc3], 1)\n",
    "                    b_merge = tf.concat([b_fc3_old, b_fc3], 0)\n",
    "                    \n",
    "                    w_expand = self.create_variable(name=\"w_ex_\"+str(task_id), shape=[self.k_ex, next_old+next_dim], scope=\"l%d\"%layer)\n",
    "                    w_expanded = tf.concat([w_merge, w_expand], 0)\n",
    "                    \n",
    "                    self.params[w_fc3_old.name] = w_expanded\n",
    "                    self.params[b_fc3_old.name] = b_merge\n",
    "                    \n",
    "                    y_conv = tf.matmul(self.h_fc2, w_expanded) + b_merge\n",
    "\n",
    "                else:\n",
    "                    w_fc2 = self.get_variable(name=\"w\",scope=\"l%d\"%layer)\n",
    "                    b_fc2 = self.get_variable(name=\"b\",scope=\"l%d\"%layer)\n",
    "\n",
    "                    prev_dim = w_fc2.get_shape().as_list()[0]\n",
    "                    next_dim = w_fc2.get_shape().as_list()[1]\n",
    "                    \n",
    "                    # Dummy nodes for prev hidden nodes\n",
    "                    dummy_w = tf.get_variable(name=\"dummy_t%d_l%d\" %(task_id,layer), shape=[self.k_ex, next_dim], \n",
    "                                initializer=tf.constant_initializer(0.0), trainable=False)\n",
    "\n",
    "                    w_expand = self.create_variable(name=\"w_ex_\"+str(task_id), shape=[prev_dim + self.k_ex, self.k_ex], scope=\"l%d\"%layer)\n",
    "                    b_expand = self.create_variable(name=\"b_ex_\"+str(task_id), shape=[self.k_ex], scope=\"l%d\"%layer)\n",
    "                    \n",
    "                    w_fc2_dummy = tf.concat([w_fc2, dummy_w],0)\n",
    "                    \n",
    "                    w_expanded = tf.concat([w_fc2_dummy, w_expand], 1)\n",
    "                    b_expanded = tf.concat([b_fc2, b_expand], 0)\n",
    "                    \n",
    "                    self.params[w_fc2.name] = w_expanded\n",
    "                    self.params[b_fc2.name] = b_expanded\n",
    "                    self.h_fc2 = tf.nn.relu(tf.matmul(h_fc1, w_expanded) + b_expanded)\n",
    "                    \n",
    "        else:\n",
    "\n",
    "            #flattened first fc layer\n",
    "            W_fc1 = self.get_variable(name=\"w\", scope=\"l1\")   # layer-1 outgoing weight matrix\n",
    "            b_fc1 = self.get_variable(name=\"b\", scope=\"l1\")\n",
    "\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(self.x, W_fc1) + b_fc1)\n",
    "\n",
    "            #second fc layer\n",
    "            W_fc2 = self.get_variable(name=\"w\", scope=\"l2\")   # layer-2 outgoing weight matrix\n",
    "            b_fc2 = self.get_variable(name=\"b\", scope=\"l2\")\n",
    "\n",
    "            self.h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "\n",
    "            self.w_fc = self.create_variable(name=\"w_\"+str(task_id), shape=[self.h_fc2.shape[1], output_len], scope=\"l3\", trainable=True)     # layer-3 outgoing weight matrix \n",
    "            self.b_fc = self.create_variable(name=\"b_\"+str(task_id), shape=[output_len], scope=\"l3\", trainable=True)\n",
    "            y_conv = tf.matmul(self.h_fc2, self.w_fc) + self.b_fc\n",
    "\n",
    "#         y_conv = tf.nn.sigmoid(y_conv)\n",
    "        return y_conv\n",
    "    \n",
    "    def perform_selection(self, task_id, values_dict):      # Breadth first search for selecting non-zero units\n",
    "        \n",
    "        all_indices = defaultdict(list)     # to store indices of nonzero units\n",
    "        selected_params = dict()            # to store values of nonzero units\n",
    "        selected_prev_params = dict()\n",
    "        \n",
    "#         for scope, value in values_dict.items():    # Storing conv layers in selected parameters \n",
    "#             if \"conv\" in scope:\n",
    "#                 selected_params[scope] = value\n",
    "            \n",
    "        for i in reversed(range(1,self.den_layers+1)):\n",
    "            if i == self.den_layers:\n",
    "                w = values_dict['l%d/w_%d:0' %(i,task_id)]\n",
    "                b = values_dict['l%d/b_%d:0' %(i,task_id)]\n",
    "                for j in range(w.shape[0]):\n",
    "                    if w[j,0] != 0:\n",
    "                        all_indices['l%d' % i].append(j)\n",
    "                # np.ix_(): fancy indexing, index with arrays of integers\n",
    "                # Select non-zero weights between the last hidden layer and the output layer\n",
    "                selected_params['l%d/w_%d:0' % (i, task_id)] = \\\n",
    "                    w[np.ix_(all_indices['l%d' % i], [0])]\n",
    "                selected_params['l%d/b_%d:0' % (i, task_id)] = b\n",
    "            else:\n",
    "                w = values_dict['l%d/w:0' % i]\n",
    "                b = values_dict['l%d/b:0' % i]\n",
    "                top_indices = all_indices['l%d' % (i + 1)]\n",
    "                print(len(top_indices))\n",
    "                for j in range(w.shape[0]):\n",
    "                    if np.count_nonzero(w[j, top_indices]) != 0 or i == 1:\n",
    "                        all_indices['l%d' % i].append(j)\n",
    "                \n",
    "                # non-zero weights between the layer i and the layer i+1\n",
    "                sub_weight = w[np.ix_(all_indices['l%d' % i], top_indices)]\n",
    "                sub_biases = b[all_indices['l%d' % (i + 1)]]\n",
    "                selected_params['l%d/w:0' % i] = sub_weight\n",
    "                selected_params['l%d/b:0' % i] = sub_biases\n",
    "                \n",
    "                # prev_W : to avoid drastic change in value of weights (Regularization)\n",
    "                selected_prev_params['l%d/w:0' % i] = \\\n",
    "                    self.prev_W['l%d/w:0' % i][np.ix_(all_indices['l%d' % i], top_indices)]\n",
    "                selected_prev_params['l%d/b:0' % i] = \\\n",
    "                    self.prev_W['l%d/b:0' % i][all_indices['l%d' % (i + 1)]]\n",
    "\n",
    "#         for keys, value in selected_params.items():\n",
    "#             print(keys)\n",
    "#             print(value)\n",
    "                \n",
    "        return [selected_params, selected_prev_params, all_indices]\n",
    "        \n",
    "    def build_SR(self, task_id, selected, output_len):    # creating selective retraining model\n",
    "        \n",
    "        self.initialize_parameters(output_len)\n",
    "        h = self.x\n",
    "        \n",
    "        for i in range(1, self.den_layers):\n",
    "            with tf.variable_scope('l%d' % i):\n",
    "                w = tf.get_variable('w', initializer=selected['l%d/w:0' % i], trainable=True)\n",
    "                b = tf.get_variable('b', initializer=selected['l%d/b:0' % i], trainable=True)\n",
    "            h = tf.nn.relu(tf.matmul(h, w) + b)\n",
    "            \n",
    "        # last layer\n",
    "        with tf.variable_scope('l%d' % self.den_layers):\n",
    "            w = tf.get_variable('w_%d' % task_id,\n",
    "                                initializer=selected['l%d/w_%d:0' % (self.den_layers, task_id)], trainable=True)\n",
    "            b = tf.get_variable('b_%d' % task_id,\n",
    "                                initializer=selected['l%d/b_%d:0' % (self.den_layers, task_id)], trainable=True)\n",
    "\n",
    "        y_conv = tf.matmul(h, w) + b\n",
    "#         y_conv = tf.nn.sigmoid(y_conv)\n",
    "        return y_conv\n",
    "\n",
    "    def optimization(self, prev_W=None):\n",
    "\n",
    "        l2_regular = 0\n",
    "        train_var = []\n",
    "        regular_terms = []\n",
    "        \n",
    "        self.loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(labels=self.y_, logits=y_conv))\n",
    "\n",
    "        for var in tf.trainable_variables():\n",
    "            l2_regular = l2_regular + tf.nn.l2_loss(var)\n",
    "            train_var.append(var)\n",
    "#         print(len(train_var))\n",
    "\n",
    "        self.loss = self.loss + tf.reduce_mean(self.l2_mu * l2_regular)\n",
    "\n",
    "        if prev_W != None:\n",
    "            for var in train_var:\n",
    "                if var.name in prev_W.keys():\n",
    "                    prev_w = prev_W[var.name]\n",
    "                    regular_terms.append(tf.nn.l2_loss(var - prev_w))\n",
    "            self.loss = self.loss + self.lamba_regular * tf.reduce_mean(regular_terms)\n",
    "        \n",
    "        opt = tf.train.AdamOptimizer(1e-5)\n",
    "        grads = opt.compute_gradients(self.loss, train_var)\n",
    "        apply_grads = opt.apply_gradients(grads)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(self.y_,1))\n",
    "        self.acc_train = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        l1_var = [var for var in tf.trainable_variables()]\n",
    "        l1_op_list = []\n",
    "        with tf.control_dependencies([apply_grads]):  # exec apply_grads first\n",
    "            for var in l1_var:\n",
    "                th_t = tf.fill(tf.shape(var), tf.convert_to_tensor(self.l1_thr))\n",
    "                zero_t = tf.zeros(tf.shape(var))\n",
    "                var_temp = var - (th_t * tf.sign(var))\n",
    "                # [pseudo]:  if |var| < th_t: var = [0];  else: var = var_temp\n",
    "                l1_op = var.assign(tf.where(tf.less(tf.abs(var), th_t), zero_t, var_temp))\n",
    "                l1_op_list.append(l1_op)\n",
    "        \n",
    "        with tf.control_dependencies(l1_op_list):\n",
    "            self.train_model = tf.no_op()\n",
    "\n",
    "    def train_task(self, task_id, batch_size, epochs, expansion=False):\n",
    "\n",
    "        if task_id == 1:\n",
    "            task_train_labels = tf.one_hot(indices=np.array(self.train_labels), depth=self.last_label_index+1)\n",
    "        elif expansion:\n",
    "            task_train_labels = tf.one_hot(indices=np.array(self.train_ex_labels), depth=self.last_label_index+1)\n",
    "        else:\n",
    "            task_train_labels = np.array(self.train_labels)\n",
    "            task_train_labels = task_train_labels.reshape((task_train_labels.shape[0],1))\n",
    "            task_train_labels = tf.convert_to_tensor(task_train_labels)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((np.array(self.train), task_train_labels))\n",
    "        dataset = dataset.shuffle(len(self.train_labels)).repeat().batch(batch_size)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        (x_data , y_data) = iterator.get_next()\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        task_train_labels = self.sess.run(task_train_labels)\n",
    "        count = 0\n",
    "\n",
    "#         for scope, ref in self.params.items():\n",
    "#             if \"l2\" in scope:\n",
    "#                 print(self.sess.run(ref))\n",
    "\n",
    "        for i in range(epochs):\n",
    "\n",
    "            for j in range(int(len(self.train)/batch_size)):\n",
    "                x_batch , y_batch = self.sess.run([x_data,y_data])\n",
    "                _, loss = self.sess.run([self.train_model, self.loss], feed_dict={self.x: x_batch, self.y_: y_batch})\n",
    "\n",
    "            train_accuracy = self.acc_train.eval(session=self.sess, feed_dict={self.x: self.train, self.y_: task_train_labels})\n",
    "            print(\"Epoch %d, training accuracy %g\"%(i+1, train_accuracy))\n",
    "\n",
    "            if train_accuracy == 1:\n",
    "                count += 1\n",
    "\n",
    "                if count > 4:\n",
    "                    print(\"Best accuracy achieved! \\n\")\n",
    "                    break\n",
    "        \n",
    "#         for scope, ref in self.params.items():\n",
    "#             if \"l2\" in scope:\n",
    "#                 print(self.sess.run(ref))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def predict(self, task_id, output_len=2):\n",
    "\n",
    "        self.initialize_parameters(output_len)\n",
    "\n",
    "        task_test_labels = tf.one_hot(indices=self.test_labels, depth=self.last_label_index+1)\n",
    "        task_test_labels = self.sess.run(task_test_labels)\n",
    "        \n",
    "        #flattened first fc layer\n",
    "        W_fc1 = self.get_variable(name=\"w\", scope=\"l1\")   # layer-1 outgoing weight matrix\n",
    "        b_fc1 = self.get_variable(name=\"b\", scope=\"l1\")\n",
    "\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(self.x, W_fc1) + b_fc1)\n",
    "\n",
    "        #second fc layer\n",
    "        W_fc2 = self.get_variable(name=\"w\", scope=\"l2\")   # layer-2 outgoing weight matrix\n",
    "        b_fc2 = self.get_variable(name=\"b\", scope=\"l2\")\n",
    "\n",
    "        self.h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "\n",
    "        self.w_fc = self.get_variable(name=\"w\", scope=\"l3\")\n",
    "        self.b_fc = self.get_variable(name=\"b\", scope=\"l3\")\n",
    "\n",
    "        y_final = tf.matmul(self.h_fc2, self.w_fc) + self.b_fc\n",
    "        y_final = tf.nn.sigmoid(y_final)\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(y_final,1), tf.argmax(self.y_,1))\n",
    "        self.acc_test = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        test_accuracy = self.acc_test.eval(session=self.sess, feed_dict={self.x: self.test, self.y_: task_test_labels})\n",
    "        print(\"Overall accuracy: %g \\n\"%test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "..........loading dataset from numpy files..........\n",
      "\n",
      " \n",
      " Added new category: Apple Braeburn\n",
      " \n",
      " Added new category: Apple Crimson Snow\n",
      "-------------Training new task: 1--------------\n",
      "Epoch 1, training accuracy 0.973291\n",
      "Epoch 2, training accuracy 1\n",
      "Epoch 3, training accuracy 1\n",
      "Epoch 4, training accuracy 1\n",
      "Best accuracy achieved! \n",
      "\n",
      "Overall accuracy: 0.887821 \n",
      "\n",
      " \n",
      " Added new category: Apple Golden 2\n",
      "936\n",
      "936\n",
      "-------------Training new task: 2--------------\n",
      "-----------Started Selective Retraining-------------\n",
      "---- Selecting nodes ----\n",
      "Epoch 1, training accuracy 1\n",
      "Epoch 2, training accuracy 1\n",
      "Epoch 3, training accuracy 1\n",
      "Best accuracy achieved! \n",
      "\n",
      "\n",
      "-----------Started Dynamic Expansion------------\n",
      "Epoch 1, training accuracy 0.921748\n",
      "Epoch 2, training accuracy 0.920732\n",
      "Epoch 3, training accuracy 0.916667\n",
      "Epoch 4, training accuracy 0.917683\n",
      "Epoch 5, training accuracy 0.914634\n",
      "Epoch 6, training accuracy 0.911585\n",
      "Epoch 7, training accuracy 0.908537\n",
      "Epoch 8, training accuracy 0.903455\n",
      "Epoch 9, training accuracy 0.895325\n",
      "Epoch 10, training accuracy 0.877033\n",
      "Epoch 11, training accuracy 0.868902\n",
      "Epoch 12, training accuracy 0.85874\n",
      "Epoch 13, training accuracy 0.846545\n",
      "Epoch 14, training accuracy 0.83435\n",
      "Epoch 15, training accuracy 0.821138\n",
      "Epoch 16, training accuracy 0.810976\n",
      "Epoch 17, training accuracy 0.792683\n",
      "Epoch 18, training accuracy 0.788618\n",
      "Epoch 19, training accuracy 0.779472\n",
      "Epoch 20, training accuracy 0.771341\n",
      "Epoch 21, training accuracy 0.76626\n",
      "Epoch 22, training accuracy 0.761179\n",
      "Epoch 23, training accuracy 0.751016\n",
      "Epoch 24, training accuracy 0.743902\n",
      "Epoch 25, training accuracy 0.742886\n",
      "Epoch 26, training accuracy 0.734756\n",
      "Epoch 27, training accuracy 0.729675\n",
      "Epoch 28, training accuracy 0.723577\n",
      "Epoch 29, training accuracy 0.719512\n",
      "Epoch 30, training accuracy 0.719512\n",
      "Epoch 31, training accuracy 0.712398\n",
      "Epoch 32, training accuracy 0.707317\n",
      "Epoch 33, training accuracy 0.701219\n",
      "Epoch 34, training accuracy 0.697154\n",
      "Epoch 35, training accuracy 0.695122\n",
      "Epoch 36, training accuracy 0.695122\n",
      "Epoch 37, training accuracy 0.694106\n",
      "Epoch 38, training accuracy 0.692073\n",
      "Epoch 39, training accuracy 0.692073\n",
      "Epoch 40, training accuracy 0.689024\n",
      "Epoch 41, training accuracy 0.689024\n",
      "Epoch 42, training accuracy 0.689024\n",
      "Epoch 43, training accuracy 0.688008\n",
      "Epoch 44, training accuracy 0.688008\n",
      "Epoch 45, training accuracy 0.688008\n",
      "Epoch 46, training accuracy 0.688008\n",
      "Epoch 47, training accuracy 0.689024\n",
      "Epoch 48, training accuracy 0.688008\n",
      "Epoch 49, training accuracy 0.688008\n",
      "Epoch 50, training accuracy 0.688008\n",
      "Overall accuracy: 0.737395 \n",
      "\n",
      " \n",
      " Added new category: Apple Golden 3\n",
      "1428\n",
      "1428\n",
      "-------------Training new task: 3--------------\n",
      "-----------Started Selective Retraining-------------\n",
      "---- Selecting nodes ----\n",
      "Epoch 1, training accuracy 1\n",
      "Epoch 2, training accuracy 1\n",
      "Epoch 3, training accuracy 1\n",
      "Best accuracy achieved! \n",
      "\n",
      "\n",
      "-----------Started Dynamic Expansion------------\n",
      "Epoch 1, training accuracy 0.398129\n",
      "Epoch 2, training accuracy 0.390852\n",
      "Epoch 3, training accuracy 0.386694\n",
      "Epoch 4, training accuracy 0.387734\n",
      "Epoch 5, training accuracy 0.387734\n",
      "Epoch 6, training accuracy 0.388773\n",
      "Epoch 7, training accuracy 0.390852\n",
      "Epoch 8, training accuracy 0.393971\n",
      "Epoch 9, training accuracy 0.39501\n",
      "Epoch 10, training accuracy 0.39605\n",
      "Epoch 11, training accuracy 0.39605\n",
      "Epoch 12, training accuracy 0.39605\n",
      "Epoch 13, training accuracy 0.39605\n",
      "Epoch 14, training accuracy 0.398129\n",
      "Epoch 15, training accuracy 0.402287\n",
      "Epoch 16, training accuracy 0.400208\n",
      "Epoch 17, training accuracy 0.401247\n",
      "Epoch 18, training accuracy 0.401247\n",
      "Epoch 19, training accuracy 0.402287\n",
      "Epoch 20, training accuracy 0.403326\n",
      "Epoch 21, training accuracy 0.403326\n",
      "Epoch 22, training accuracy 0.403326\n",
      "Epoch 23, training accuracy 0.403326\n",
      "Epoch 24, training accuracy 0.403326\n",
      "Epoch 25, training accuracy 0.403326\n",
      "Epoch 26, training accuracy 0.403326\n",
      "Epoch 27, training accuracy 0.403326\n",
      "Epoch 28, training accuracy 0.403326\n",
      "Epoch 29, training accuracy 0.403326\n",
      "Epoch 30, training accuracy 0.403326\n",
      "Epoch 31, training accuracy 0.403326\n",
      "Epoch 32, training accuracy 0.403326\n",
      "Epoch 33, training accuracy 0.403326\n",
      "Epoch 34, training accuracy 0.403326\n",
      "Epoch 35, training accuracy 0.404366\n",
      "Epoch 36, training accuracy 0.404366\n",
      "Epoch 37, training accuracy 0.405405\n",
      "Epoch 38, training accuracy 0.405405\n",
      "Epoch 39, training accuracy 0.406445\n",
      "Epoch 40, training accuracy 0.406445\n",
      "Epoch 41, training accuracy 0.406445\n",
      "Epoch 42, training accuracy 0.407484\n",
      "Epoch 43, training accuracy 0.407484\n",
      "Epoch 44, training accuracy 0.407484\n",
      "Epoch 45, training accuracy 0.407484\n",
      "Epoch 46, training accuracy 0.407484\n",
      "Epoch 47, training accuracy 0.408524\n",
      "Epoch 48, training accuracy 0.404366\n",
      "Epoch 49, training accuracy 0.406445\n",
      "Epoch 50, training accuracy 0.404366\n",
      "Overall accuracy: 0.55259 \n",
      "\n",
      " \n",
      " Added new category: Apricot\n",
      "1909\n",
      "1909\n",
      "-------------Training new task: 4--------------\n",
      "-----------Started Selective Retraining-------------\n",
      "---- Selecting nodes ----\n",
      "Epoch 1, training accuracy 1\n",
      "Epoch 2, training accuracy 1\n",
      "Epoch 3, training accuracy 1\n",
      "Best accuracy achieved! \n",
      "\n",
      "\n",
      "-----------Started Dynamic Expansion------------\n",
      "Epoch 1, training accuracy 0.306911\n",
      "Epoch 2, training accuracy 0.301829\n",
      "Epoch 3, training accuracy 0.292683\n",
      "Epoch 4, training accuracy 0.285569\n",
      "Epoch 5, training accuracy 0.281504\n",
      "Epoch 6, training accuracy 0.278455\n",
      "Epoch 7, training accuracy 0.277439\n",
      "Epoch 8, training accuracy 0.276423\n",
      "Epoch 9, training accuracy 0.275407\n",
      "Epoch 10, training accuracy 0.275407\n",
      "Epoch 11, training accuracy 0.276423\n",
      "Epoch 12, training accuracy 0.276423\n",
      "Epoch 13, training accuracy 0.280488\n",
      "Epoch 14, training accuracy 0.292683\n",
      "Epoch 15, training accuracy 0.295732\n",
      "Epoch 16, training accuracy 0.304878\n",
      "Epoch 17, training accuracy 0.314024\n",
      "Epoch 18, training accuracy 0.321138\n",
      "Epoch 19, training accuracy 0.323171\n",
      "Epoch 20, training accuracy 0.329268\n",
      "Epoch 21, training accuracy 0.341463\n",
      "Epoch 22, training accuracy 0.353659\n",
      "Epoch 23, training accuracy 0.359756\n",
      "Epoch 24, training accuracy 0.363821\n",
      "Epoch 25, training accuracy 0.370935\n",
      "Epoch 26, training accuracy 0.373984\n",
      "Epoch 27, training accuracy 0.376016\n",
      "Epoch 28, training accuracy 0.373984\n",
      "Epoch 29, training accuracy 0.375\n",
      "Epoch 30, training accuracy 0.377033\n",
      "Epoch 31, training accuracy 0.378049\n",
      "Epoch 32, training accuracy 0.380081\n",
      "Epoch 33, training accuracy 0.385163\n",
      "Epoch 34, training accuracy 0.386179\n",
      "Epoch 35, training accuracy 0.386179\n",
      "Epoch 36, training accuracy 0.387195\n",
      "Epoch 37, training accuracy 0.388211\n",
      "Epoch 38, training accuracy 0.389228\n",
      "Epoch 39, training accuracy 0.388211\n",
      "Epoch 40, training accuracy 0.390244\n",
      "Epoch 41, training accuracy 0.390244\n",
      "Epoch 42, training accuracy 0.390244\n",
      "Epoch 43, training accuracy 0.390244\n",
      "Epoch 44, training accuracy 0.392276\n",
      "Epoch 45, training accuracy 0.392276\n",
      "Epoch 46, training accuracy 0.392276\n",
      "Epoch 47, training accuracy 0.394309\n",
      "Epoch 48, training accuracy 0.393293\n",
      "Epoch 49, training accuracy 0.394309\n",
      "Epoch 50, training accuracy 0.395325\n",
      "Overall accuracy: 0.362047 \n",
      "\n",
      " \n",
      " Added new category: Avocado\n",
      "2401\n",
      "2401\n",
      "-------------Training new task: 5--------------\n",
      "-----------Started Selective Retraining-------------\n",
      "---- Selecting nodes ----\n",
      "Epoch 1, training accuracy 1\n",
      "Epoch 2, training accuracy 1\n",
      "Epoch 3, training accuracy 1\n",
      "Best accuracy achieved! \n",
      "\n",
      "\n",
      "-----------Started Dynamic Expansion------------\n",
      "Epoch 1, training accuracy 0.251756\n",
      "Epoch 2, training accuracy 0.265808\n",
      "Epoch 3, training accuracy 0.265808\n",
      "Epoch 4, training accuracy 0.271663\n",
      "Epoch 5, training accuracy 0.277518\n",
      "Epoch 6, training accuracy 0.286885\n",
      "Epoch 7, training accuracy 0.314988\n",
      "Epoch 8, training accuracy 0.351288\n",
      "Epoch 9, training accuracy 0.367682\n",
      "Epoch 10, training accuracy 0.382904\n",
      "Epoch 11, training accuracy 0.413349\n",
      "Epoch 12, training accuracy 0.421546\n",
      "Epoch 13, training accuracy 0.423888\n",
      "Epoch 14, training accuracy 0.436768\n",
      "Epoch 15, training accuracy 0.443794\n",
      "Epoch 16, training accuracy 0.447307\n",
      "Epoch 17, training accuracy 0.4637\n",
      "Epoch 18, training accuracy 0.490632\n",
      "Epoch 19, training accuracy 0.498829\n",
      "Epoch 20, training accuracy 0.512881\n",
      "Epoch 21, training accuracy 0.530445\n",
      "Epoch 22, training accuracy 0.540984\n",
      "Epoch 23, training accuracy 0.555035\n",
      "Epoch 24, training accuracy 0.571429\n",
      "Epoch 25, training accuracy 0.571429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, training accuracy 0.578454\n",
      "Epoch 27, training accuracy 0.583138\n",
      "Epoch 28, training accuracy 0.584309\n",
      "Epoch 29, training accuracy 0.591335\n",
      "Epoch 30, training accuracy 0.59719\n",
      "Epoch 31, training accuracy 0.598361\n",
      "Epoch 32, training accuracy 0.601874\n",
      "Epoch 33, training accuracy 0.615925\n",
      "Epoch 34, training accuracy 0.620609\n",
      "Epoch 35, training accuracy 0.624122\n",
      "Epoch 36, training accuracy 0.615925\n",
      "Epoch 37, training accuracy 0.632318\n",
      "Epoch 38, training accuracy 0.63466\n",
      "Epoch 39, training accuracy 0.637002\n",
      "Epoch 40, training accuracy 0.639344\n",
      "Epoch 41, training accuracy 0.638173\n",
      "Epoch 42, training accuracy 0.640515\n",
      "Epoch 43, training accuracy 0.639344\n",
      "Epoch 44, training accuracy 0.642857\n",
      "Epoch 45, training accuracy 0.641686\n",
      "Epoch 46, training accuracy 0.644028\n",
      "Epoch 47, training accuracy 0.64637\n",
      "Epoch 48, training accuracy 0.647541\n",
      "Epoch 49, training accuracy 0.653396\n",
      "Epoch 50, training accuracy 0.654567\n",
      "Overall accuracy: 0.580508 \n",
      "\n",
      " \n",
      " Added new category: Avocado ripe\n",
      "2828\n",
      "2828\n",
      "-------------Training new task: 6--------------\n",
      "-----------Started Selective Retraining-------------\n",
      "---- Selecting nodes ----\n",
      "Epoch 1, training accuracy 1\n",
      "Epoch 2, training accuracy 1\n",
      "Epoch 3, training accuracy 1\n",
      "Best accuracy achieved! \n",
      "\n",
      "\n",
      "-----------Started Dynamic Expansion------------\n",
      "Epoch 1, training accuracy 0.39002\n",
      "Epoch 2, training accuracy 0.39613\n",
      "Epoch 3, training accuracy 0.397149\n",
      "Epoch 4, training accuracy 0.400204\n",
      "Epoch 5, training accuracy 0.40224\n",
      "Epoch 6, training accuracy 0.404277\n",
      "Epoch 7, training accuracy 0.404277\n",
      "Epoch 8, training accuracy 0.406314\n",
      "Epoch 9, training accuracy 0.40835\n",
      "Epoch 10, training accuracy 0.40835\n",
      "Epoch 11, training accuracy 0.40835\n",
      "Epoch 12, training accuracy 0.409369\n",
      "Epoch 13, training accuracy 0.409369\n",
      "Epoch 14, training accuracy 0.412424\n",
      "Epoch 15, training accuracy 0.412424\n",
      "Epoch 16, training accuracy 0.412424\n",
      "Epoch 17, training accuracy 0.413442\n",
      "Epoch 18, training accuracy 0.415479\n",
      "Epoch 19, training accuracy 0.415479\n",
      "Epoch 20, training accuracy 0.419552\n",
      "Epoch 21, training accuracy 0.419552\n",
      "Epoch 22, training accuracy 0.421589\n",
      "Epoch 23, training accuracy 0.423625\n",
      "Epoch 24, training accuracy 0.423625\n",
      "Epoch 25, training accuracy 0.423625\n",
      "Epoch 26, training accuracy 0.425662\n",
      "Epoch 27, training accuracy 0.425662\n",
      "Epoch 28, training accuracy 0.42668\n",
      "Epoch 29, training accuracy 0.423625\n",
      "Epoch 30, training accuracy 0.423625\n",
      "Epoch 31, training accuracy 0.424644\n",
      "Epoch 32, training accuracy 0.424644\n",
      "Epoch 33, training accuracy 0.424644\n",
      "Epoch 34, training accuracy 0.424644\n",
      "Epoch 35, training accuracy 0.423625\n",
      "Epoch 36, training accuracy 0.424644\n",
      "Epoch 37, training accuracy 0.423625\n",
      "Epoch 38, training accuracy 0.422607\n",
      "Epoch 39, training accuracy 0.423625\n",
      "Epoch 40, training accuracy 0.424644\n",
      "Epoch 41, training accuracy 0.422607\n",
      "Epoch 42, training accuracy 0.422607\n",
      "Epoch 43, training accuracy 0.421589\n",
      "Epoch 44, training accuracy 0.421589\n",
      "Epoch 45, training accuracy 0.421589\n",
      "Epoch 46, training accuracy 0.421589\n",
      "Epoch 47, training accuracy 0.419552\n",
      "Epoch 48, training accuracy 0.419552\n",
      "Epoch 49, training accuracy 0.419552\n",
      "Epoch 50, training accuracy 0.418534\n",
      "Overall accuracy: 0.474775 \n",
      "\n",
      " \n",
      " Added new category: Banana Lady Finger\n",
      "3319\n",
      "3319\n",
      "-------------Training new task: 7--------------\n",
      "-----------Started Selective Retraining-------------\n",
      "---- Selecting nodes ----\n",
      "Epoch 1, training accuracy 1\n",
      "Epoch 2, training accuracy 1\n",
      "Epoch 3, training accuracy 1\n",
      "Best accuracy achieved! \n",
      "\n",
      "\n",
      "-----------Started Dynamic Expansion------------\n",
      "Epoch 1, training accuracy 0.563333\n",
      "Epoch 2, training accuracy 0.594444\n",
      "Epoch 3, training accuracy 0.613333\n",
      "Epoch 4, training accuracy 0.612222\n",
      "Epoch 5, training accuracy 0.608889\n",
      "Epoch 6, training accuracy 0.612222\n",
      "Epoch 7, training accuracy 0.618889\n",
      "Epoch 8, training accuracy 0.621111\n",
      "Epoch 9, training accuracy 0.622222\n",
      "Epoch 10, training accuracy 0.627778\n",
      "Epoch 11, training accuracy 0.627778\n",
      "Epoch 12, training accuracy 0.633333\n",
      "Epoch 13, training accuracy 0.638889\n",
      "Epoch 14, training accuracy 0.645556\n",
      "Epoch 15, training accuracy 0.653333\n",
      "Epoch 16, training accuracy 0.656667\n",
      "Epoch 17, training accuracy 0.663333\n",
      "Epoch 18, training accuracy 0.67\n",
      "Epoch 19, training accuracy 0.676667\n",
      "Epoch 20, training accuracy 0.678889\n",
      "Epoch 21, training accuracy 0.678889\n",
      "Epoch 22, training accuracy 0.678889\n",
      "Epoch 23, training accuracy 0.681111\n",
      "Epoch 24, training accuracy 0.683333\n",
      "Epoch 25, training accuracy 0.683333\n",
      "Epoch 26, training accuracy 0.687778\n",
      "Epoch 27, training accuracy 0.687778\n",
      "Epoch 28, training accuracy 0.688889\n",
      "Epoch 29, training accuracy 0.691111\n",
      "Epoch 30, training accuracy 0.693333\n",
      "Epoch 31, training accuracy 0.693333\n",
      "Epoch 32, training accuracy 0.698889\n",
      "Epoch 33, training accuracy 0.7\n",
      "Epoch 34, training accuracy 0.7\n",
      "Epoch 35, training accuracy 0.697778\n",
      "Epoch 36, training accuracy 0.701111\n",
      "Epoch 37, training accuracy 0.702222\n",
      "Epoch 38, training accuracy 0.703333\n",
      "Epoch 39, training accuracy 0.702222\n",
      "Epoch 40, training accuracy 0.701111\n",
      "Epoch 41, training accuracy 0.7\n",
      "Epoch 42, training accuracy 0.7\n",
      "Epoch 43, training accuracy 0.701111\n",
      "Epoch 44, training accuracy 0.701111\n",
      "Epoch 45, training accuracy 0.704444\n",
      "Epoch 46, training accuracy 0.705556\n",
      "Epoch 47, training accuracy 0.708889\n",
      "Epoch 48, training accuracy 0.712222\n",
      "Epoch 49, training accuracy 0.713333\n",
      "Epoch 50, training accuracy 0.714444\n",
      "Overall accuracy: 0.420761 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # train_data_path = '../Fruit_dataset/Train'\n",
    "    # test_data_path = '../Fruit_dataset/Test'\n",
    "\n",
    "    # lesser categories for faster loading during testing the code\n",
    "    train_data_path = '../Fruit_dataset/temp_Train'\n",
    "    test_data_path = '../Fruit_dataset/temp_Test'\n",
    "\n",
    "    # For easier disk read operation\n",
    "    np_train_path = '../Fruit_dataset/numpy_dataset/train.npy.gz'\n",
    "    np_test_path = '../Fruit_dataset/numpy_dataset/test.npy.gz'\n",
    "    np_train_label_path = '../Fruit_dataset/numpy_dataset/train_labels.npy'\n",
    "    np_test_label_path = '../Fruit_dataset/numpy_dataset/test_labels.npy'\n",
    "    np_label_name_path = '../Fruit_dataset/numpy_dataset/label_names.npy'\n",
    "\n",
    "    batch_size = 10\n",
    "    epochs = 50\n",
    "    early_stop = 5\n",
    "\n",
    "    den = DEN()\n",
    "\n",
    "    if os.path.exists(np_train_path):\n",
    "\n",
    "        print(\"\\n..........loading dataset from numpy files..........\\n\")\n",
    "\n",
    "        with gzip.GzipFile(np_train_path, \"r\") as f:\n",
    "            train_data = np.load(f)\n",
    "        with gzip.GzipFile(np_test_path, \"r\") as f:\n",
    "            test_data = np.load(f)\n",
    "\n",
    "        train_labels = np.load(np_train_label_path)\n",
    "        test_labels = np.load(np_test_label_path)\n",
    "        label_names = np.load(np_label_name_path)\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(\"\\n..........loading dataset from disk..........\\n\")\n",
    "        train_data, train_labels, label_names = den.extract_data(train_data_path)\n",
    "        test_data, test_labels, _ = den.extract_data(test_data_path)\n",
    "\n",
    "        os.makedirs(os.path.dirname(np_train_path), exist_ok=True)\n",
    "\n",
    "        with gzip.GzipFile(np_train_path, \"w\") as f:\n",
    "            np.save(f, train_data)\n",
    "        with gzip.GzipFile(np_test_path, \"w\") as f:\n",
    "            np.save(f, test_data)\n",
    "\n",
    "        np.save(np_train_label_path, train_labels)\n",
    "        np.save(np_test_label_path, test_labels)\n",
    "        np.save(np_label_name_path, label_names)\n",
    "\n",
    "\n",
    "# show image using cv\n",
    "    # print(train_labels[512])\n",
    "    # cv.imshow(\"\", train_data[512])\n",
    "    # cv.waitKey(0)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    task_id = 0\n",
    "    y_conv = None\n",
    "\n",
    "    while den.last_label_index != (len(label_names) - 1):     # Loop for adding new tasks (lifelong learning)\n",
    "\n",
    "        task_id += 1\n",
    "        den.add_task(task_id, label_names)\n",
    "        param_values = dict()\n",
    "        selected = dict()\n",
    "        print(\"-------------Training new task: %d--------------\"%task_id)\n",
    "        if task_id == 1:\n",
    "            y_conv = den.build_model(task_id=task_id)\n",
    "            den.optimization()\n",
    "            _ = den.train_task(task_id=task_id, batch_size=batch_size, epochs=epochs)\n",
    "            params = den.get_params()\n",
    "        else:\n",
    "            print(\"-----------Started Selective Retraining-------------\")\n",
    "            \n",
    "            #------------------Selection-------------------\n",
    "            y_conv = den.build_model(task_id=task_id, output_len=1)\n",
    "            den.sess.run(tf.global_variables_initializer())\n",
    "            params = den.get_params()\n",
    "            den.optimization()\n",
    "            \n",
    "            print(\"---- Selecting nodes ----\")\n",
    "            _ = den.train_task(task_id=task_id, batch_size=batch_size, epochs=early_stop)\n",
    "            params = den.get_params()\n",
    "            if False:           \n",
    "                [selected, selected_prev, all_indices] = den.perform_selection(task_id=task_id, values_dict=params)\n",
    "                den.destroy_graph()\n",
    "\n",
    "                #------------------Retraining-------------------\n",
    "                print(\"---- Retraining selected nodes ----\")\n",
    "                y_conv = den.build_SR(task_id=task_id, selected=selected, output_len=1) \n",
    "                den.optimization(prev_W=selected_prev) \n",
    "                loss = den.train_task(task_id=task_id, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "                print(\"Loss: %f\"%loss)\n",
    "            \n",
    "#             if loss < den.loss_thr:\n",
    "\n",
    "            \n",
    "                #--------------Performing Union----------------\n",
    "                _vars = [(var.name, den.sess.run(var)) for var in tf.trainable_variables() if 'l' in var.name]\n",
    "\n",
    "                for item in _vars:\n",
    "                    key, values = item\n",
    "                    selected[key] = values\n",
    "\n",
    "                for i in reversed(range(1, den.den_layers+1)):\n",
    "                    if i == den.den_layers:\n",
    "                        temp_weight = params['l%d/w_%d:0' % (i, task_id)]\n",
    "                        temp_weight[np.ix_(all_indices['l%d' % i], [0])] = \\\n",
    "                            selected['l%d/w_%d:0' % (i, task_id)]\n",
    "                        params['l%d/w_%d:0' % (i, task_id)] = temp_weight\n",
    "                        params['l%d/b_%d:0' % (i, task_id)] = \\\n",
    "                            selected['l%d/b_%d:0' % (i, task_id)]\n",
    "                        # Updating output matrix structure\n",
    "                        params['l%d/w:0' % (i)] = np.concatenate([params['l%d/w:0' % (i)],params['l%d/w_%d:0' % (i,task_id)]], axis=1).tolist()\n",
    "                        params['l%d/b:0' % (i)] = np.concatenate([params['l%d/b:0' % (i)],params['l%d/b_%d:0' % (i,task_id)]], axis=0).tolist()\n",
    "                    else:\n",
    "                        temp_weight = params['l%d/w:0' % i]\n",
    "                        temp_biases = params['l%d/b:0' % i]\n",
    "                        temp_weight[np.ix_(all_indices['l%d' % i], all_indices['l%d' % (i + 1)])] = \\\n",
    "                            selected['l%d/w:0' % i]\n",
    "                        temp_biases[all_indices['l%d' % (i + 1)]] = \\\n",
    "                            selected['l%d/b:0' % i]\n",
    "                        params['l%d/w:0' % i] = temp_weight\n",
    "                        params['l%d/b:0' % i] = temp_biases\n",
    "\n",
    "            else:\n",
    "                \n",
    "                print(\"\\n-----------Started Dynamic Expansion------------\")\n",
    "                den.destroy_graph()\n",
    "                den.restore_params(task_id=task_id, trainable=False, param_values=params)\n",
    "                y_conv = den.build_model(task_id=task_id, expansion=True, output_len=den.last_label_index+1)\n",
    "                den.optimization()\n",
    "                _ = den.train_task(task_id=task_id, batch_size=batch_size, epochs=epochs, expansion=True)\n",
    "                params = den.get_params()\n",
    "#                 print(params)\n",
    "#                 params['l%d/w:0' % den.den_layers] = np.concatenate([params['l%d/w:0' % den.den_layers],params['l%d/w_%d:0' % (den.den_layers,task_id)]], axis=1).tolist()\n",
    "#                 params['l%d/b:0' % den.den_layers] = np.concatenate([params['l%d/b:0' % den.den_layers],params['l%d/b_%d:0' % (den.den_layers,task_id)]], axis=0).tolist()\n",
    "\n",
    "        den.destroy_graph()\n",
    "        den.restore_params(trainable=False, param_values=params)    # Freezes all learned weights\n",
    "                    \n",
    "        # ---------Performance-----------\n",
    "        den.predict(task_id=task_id, output_len=den.last_label_index+1)\n",
    "                    \n",
    "#         param_values = den.get_params()         # Backing-up model weights that were trained upto current task\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
